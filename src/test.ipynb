{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d442606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import argparse\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import outlines\n",
    "from outlines.types.dsl import Regex, String, at_most, either, to_regex\n",
    "from outlines.types import zero_or_more, one_or_more, optional, whitespace, digit\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from pydantic_evals import Dataset\n",
    "from pydantic_evals.evaluators import Evaluator, EvaluatorContext\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from m_gsm_symbolic.kaenguruen.load_data import load_kaenguruen\n",
    "from m_gsm_symbolic.load_data import load_gsm_dan, load_gsm_eng\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54ab570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c58e0985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "#load_dotenv()\n",
    "#api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "from huggingface_hub import login\n",
    "#login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5dc05c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#judge_llm_name = \"openai:gpt-4o-2024-08-06\"\n",
    "response_model_name = \"meta-llama/Llama-3.2-1B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee20a52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Outlines regex DSL: reasoning (any text) + '####' + answer (int)\n",
    "#answer_pattern = to_regex(\n",
    "#    Regex(pattern=r\".\").at_most(500) +\n",
    "#    String(\"#### \") +\n",
    "#    digit.one_or_more() +\n",
    "#    optional(\n",
    "#        either(\n",
    "#            String(\".\"), String(\",\")\n",
    "#        ) + optional(\n",
    "#            digit.one_or_more()\n",
    "#        )\n",
    "#    )\n",
    "#)\n",
    "answer_pattern = Regex(r\".{,300}####\\s\\d+\")\n",
    "# Support float or frac as well\n",
    "    #sequence(\n",
    "    #    one_or_more(digit()),\n",
    "    #    optional(either(\n",
    "    #        sequence(literal(\".\"), one_or_more(digit())),\n",
    "    #        sequence(literal(\"/\"), one_or_more(digit()))\n",
    "    #    ))\n",
    "    #)\n",
    "\n",
    "class HuggingFaceAgent:\n",
    "    def __init__(self, model: str, examples: list, \n",
    "                 answer_pattern: Regex = answer_pattern):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        self.model = outlines.from_transformers(AutoModelForCausalLM.from_pretrained(model, torch_dtype=\"auto\").to(device), self.tokenizer)\n",
    "        self.cases = examples\n",
    "        self.answer_pattern = answer_pattern\n",
    "\n",
    "    def _build_prompt(self, prompt):\n",
    "        examples = []\n",
    "        for case in self.cases:\n",
    "            example = f\"Problem: {case.inputs}\\n\\nSolution: {case.expected_output}\"\n",
    "            examples.append(example)\n",
    "\n",
    "        prompt = f\"Problem: {prompt}\\n\\nSolution:\"\n",
    "        examples.append(prompt)\n",
    "        prompt = \"\\n\\n\".join(examples)\n",
    "        return prompt\n",
    "\n",
    "    def run(self, prompt: str):\n",
    "        prompt = self._build_prompt(prompt)\n",
    "        model_output = self.model(prompt, answer_pattern, max_new_tokens=500)\n",
    "        #model_output = self.model(prompt, max_new_tokens=500\n",
    "        return model_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0143d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 0: 40\n",
      "Case 1: 58\n",
      "Case 2: 30\n",
      "Case 3: 38\n",
      "Case 4: 56\n",
      "Case 5: 4\n",
      "Case 6: 18\n",
      "Case 7: 54\n",
      "Case 8: 210\n",
      "Case 9: 1050\n",
      "Case 10: 8\n",
      "Case 11: 4\n",
      "Case 12: 5\n",
      "Case 13: 55\n",
      "Case 14: 2\n",
      "Case 15: 11\n",
      "Case 16: 77\n",
      "Case 17: 118000\n",
      "Case 18: 16\n",
      "Case 19: 540\n",
      "Case 20: 35\n",
      "Case 21: 43200\n",
      "Case 22: 64\n",
      "Case 23: 12\n",
      "Case 24: 88\n",
      "Case 25: 10\n",
      "Case 26: 130\n",
      "Case 27: 342\n",
      "Case 28: 70\n",
      "Case 29: 320\n",
      "Case 30: 157\n",
      "Case 31: 2\n",
      "Case 32: 25\n",
      "Case 33: 72\n",
      "Case 34: 75\n",
      "Case 35: 140\n",
      "Case 36: 16\n",
      "Case 37: 14400\n",
      "Case 38: 2\n",
      "Case 39: 3200\n",
      "Case 40: 100\n",
      "Case 41: 5\n",
      "Case 42: 14\n",
      "Case 43: 4\n",
      "Case 44: 63\n",
      "Case 45: 2\n",
      "Case 46: 15\n",
      "Case 47: 200\n",
      "Case 48: 4\n",
      "Case 49: 694\n",
      "Case 50: 4000\n",
      "Case 51: 6\n",
      "Case 52: 9\n",
      "Case 53: 159\n",
      "Case 54: 48\n",
      "Case 55: 45\n",
      "Case 56: 100\n",
      "Case 57: 3\n",
      "Case 58: 42\n",
      "Case 59: 48\n",
      "Case 60: 428\n",
      "Case 61: 25\n",
      "Case 62: 75\n",
      "Case 63: 10\n",
      "Case 64: 3430\n",
      "Case 65: 18\n",
      "Case 66: 78\n",
      "Case 67: 16\n",
      "Case 68: 8\n",
      "Case 69: 49\n",
      "Case 70: 3\n",
      "Case 71: 20\n",
      "Case 72: 34\n",
      "Case 73: 36\n",
      "Case 74: 48\n",
      "Case 75: 1248\n",
      "Case 76: 9500\n",
      "Case 77: 16\n",
      "Case 78: 2\n",
      "Case 79: 36\n",
      "Case 80: 98\n",
      "Case 81: 30\n",
      "Case 82: 44\n",
      "Case 83: 30\n",
      "Case 84: 7\n",
      "Case 85: 78\n",
      "Case 86: 2\n",
      "Case 87: 19\n",
      "Case 88: 2325\n",
      "Case 89: 13\n",
      "Case 90: 750\n",
      "Case 91: 12\n",
      "Case 92: 22\n",
      "Case 93: 20\n",
      "Case 94: 15\n",
      "Case 95: 20\n",
      "Case 96: 10\n",
      "Case 97: 50\n",
      "Case 98: 11\n",
      "Case 99: 50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cases = [p.to_case() for p in load_gsm_eng()]\n",
    "\n",
    "for i, case in enumerate(cases):\n",
    "    print(f\"Case {i}: {re.search(r'####\\s*((\\d|,)+)', case.expected_output).group(1).replace(',', '')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8729d50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = [p.to_case() for p in load_gsm_eng()]\n",
    "\n",
    "response_model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "agent_evaluated = HuggingFaceAgent(response_model_name, examples=cases[-3:], answer_pattern=answer_pattern)\n",
    "\n",
    "# Custom evaluator: compare only the answer after '####'\n",
    "@dataclass\n",
    "class AnswerOnlyMatch(Evaluator):\n",
    "    def evaluate(self, ctx: EvaluatorContext) -> bool:\n",
    "        # Extract answer after '####' using regex\n",
    "        m_pred = re.search(r\"####\\s*((\\d|,)+)\", ctx.output)\n",
    "        m_true = re.search(r\"####\\s*((\\d|,)+)\", ctx.expected_output)\n",
    "        if not m_pred or not m_true:\n",
    "            return False\n",
    "        return float(m_pred.group(1).replace(\",\",\"\")) == float(m_true.group(1).replace(\",\",\"\"))\n",
    "\n",
    "ds = Dataset(\n",
    "    cases=cases[:2],\n",
    "    evaluators=[AnswerOnlyMatch()],\n",
    ")\n",
    "\n",
    "async def answer_question(question: str) -> str:\n",
    "    r = agent_evaluated.run(question)\n",
    "    return r\n",
    "\n",
    "report = ds.evaluate_sync(answer_question)\n",
    "report.print(include_input=True, include_output=True, include_expected_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "209d5cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case:\n",
      "Case(name='661', inputs=\"Roger goes to the store to buy some coffee. The normal brand of coffee he buys costs $5 per pound. He had to buy a more expensive brand that costs 20% more since his favorite brand was sold out. He decides to buy a week's worth of coffee and he uses 1 pound of coffee per day. He also decided to buy himself a donut for $2. How much did everything cost?\", metadata={'filepath': '/home/simonenni/repos/m-gsm-symbolic/data/templates/eng/symbolic/0042.json'}, expected_output='The coffee he bought was 5*0.2=$<<5*0.2=1>>1 more expensive per pound than what he normally buys\\nSo it cost 5+1=$<<5+1=6>>6 per pound\\nHe goes through 1*7=<<1*7=7>>7 pounds of coffee a week\\nSo he paid 6*7=$<<6*7=42>>42 on coffee\\nThat means his total bill was 42+2=$<<42+2=44>>44\\n#### 44', evaluators=[])\n",
      "Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Roger bought 20% more coffee than he needed because he bought a week's worth of coffee. So, 1 pound of coffee costs $5. 20% of $5 is $1. So, 1 pound of coffee costs $1. So, 1 pound of coffee costs $5. So, 1 pound of coffee costs $5. So, 1 pound of coffee costs $5. So, 1 pound of coffee costs $5. So#### 1\n",
      "Case:\n",
      "Case(name='737', inputs='Peter wants to make different sized ice cubes with 32 ounces of water. He can make giant cubes that use 4 ounces per cube, medium cubes that use 2 ounces, and small cubes that use 1/2 ounce. If he makes 3 giant cubes, 7 medium cubes, and 8 small cubes, how many ounces of water does he have left?', metadata={'filepath': '/home/simonenni/repos/m-gsm-symbolic/data/templates/eng/symbolic/0011.json'}, expected_output='The giant cubes used up 12 ounces of water because 3 times 4 equals <<3*4=12>>12.\\nThe medium cubes used up 14 ounces of water because 7 times 2 equals <<7*2=14>>14.\\nThe small cubes used up 4 ounces of water because 8 times 1/2 equals 4.\\nThis means that Peter has used up 30 ounces of water because 12 plus 14 plus 4 equals 30.\\nPeter has 2 ounces of water left because 32 minus 30 equals <<32-30=2>>2.\\n#### 2', evaluators=[])\n",
      "Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32 ounces of water divided by 4 = <<32 ounces/4=8>>8 ounces of water left over for other uses. 32 ounces of water divided by 2 = <<32 ounces/2=16>>16 ounces of water left over for other uses. 32 ounces of water divided by 1/2 = <<32 ounces/1/2=16>>16 ounces of water left over for other uses. 16 + 8#### 24\n"
     ]
    }
   ],
   "source": [
    "cases = [p.to_case() for p in load_gsm_eng()]\n",
    "response_model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "agent_evaluated = HuggingFaceAgent(response_model_name, examples=cases[-3:], answer_pattern=answer_pattern)\n",
    "agent_evaluated.model\n",
    "for case in cases[:2]:\n",
    "    print(\"Case:\")\n",
    "    print(case)\n",
    "    print(\"Output:\")\n",
    "    print(agent_evaluated.run(case.inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d754afea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<outlines.models.transformers.Transformers at 0x70a6f4392e10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_evaluated.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c097bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", torch_dtype=\"auto\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56655ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3474a8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Case(name='728', inputs='Candy has 15 light blue spools of thread, 45 dark blue spools of thread, 40 light green spools of thread, and 50 dark green spools of thread. What percent of her spools are blue?', metadata={'filepath': '/home/au338890/repos/m-gsm-symbolic/data/templates/eng/symbolic/0066.json'}, expected_output='First find the number of blue spools: 15 spools + 45 spools = <<15+45=60>>60 spools\\nThen find the total number of spools: 40 spools + 50 spools + 60 spools = <<40+50+60=150>>150 spools\\nThen divide the number of blue spools by the total number of spools and multiply by 100% to express the answer as a percentage: 60 spools / 150 spools * 100% = 40%\\n#### 40', evaluators=[])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b61d6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_evaluated = HuggingFaceAgent(response_model_name, examples=cases[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "393b0bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3672, in run_code\n",
      "  |     exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  |     ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/tmp/ipykernel_10327/1812324380.py\", line 5, in <module>\n",
      "  |     report_2 = ds.evaluate_sync(answer_question)\n",
      "  |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 315, in evaluate_sync\n",
      "  |     return get_event_loop().run_until_complete(self.evaluate(task, name=name, max_concurrency=max_concurrency))\n",
      "  |            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/nest_asyncio.py\", line 98, in run_until_complete\n",
      "  |     return f.result()\n",
      "  |            ~~~~~~~~^^\n",
      "  |   File \"/home/au338890/.local/share/uv/python/cpython-3.13.3-linux-x86_64-gnu/lib/python3.13/asyncio/futures.py\", line 199, in result\n",
      "  |     raise self._exception.with_traceback(self._exception_tb)\n",
      "  |   File \"/home/au338890/.local/share/uv/python/cpython-3.13.3-linux-x86_64-gnu/lib/python3.13/asyncio/tasks.py\", line 306, in __step_run_and_handle_result\n",
      "  |     result = coro.throw(exc)\n",
      "  |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 283, in evaluate\n",
      "  |     cases=await task_group_gather(\n",
      "  |           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |     ...<4 lines>...\n",
      "  |     ),\n",
      "  |     ^\n",
      "  |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/_utils.py\", line 99, in task_group_gather\n",
      "  |     async with anyio.create_task_group() as tg:\n",
      "  |                ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  |         \"unhandled errors in a TaskGroup\", self._exceptions\n",
      "  |     ) from None\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (2 sub-exceptions)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/home/au338890/.local/share/uv/python/cpython-3.13.3-linux-x86_64-gnu/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
      "    |     result = coro.send(None)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/_utils.py\", line 97, in _run_task\n",
      "    |     results[index] = await tsk()\n",
      "    |                      ^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 279, in _handle_case\n",
      "    |     return await _run_task_and_evaluators(task, case, report_case_name, self.evaluators)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 882, in _run_task_and_evaluators\n",
      "    |     scoring_context = await _run_task(task, case)\n",
      "    |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 820, in _run_task\n",
      "    |     task_output = await task(case.inputs)\n",
      "    |                   ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/tmp/ipykernel_10327/1812324380.py\", line 2, in answer_question\n",
      "    |     r = agent_evaluated.run(question)\n",
      "    |   File \"/tmp/ipykernel_10327/3741609218.py\", line 25, in run\n",
      "    |     model_output = self.model.generate(**model_input, max_new_tokens=500)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    |     return func(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/generation/utils.py\", line 2633, in generate\n",
      "    |     result = self._sample(\n",
      "    |         input_ids,\n",
      "    |     ...<5 lines>...\n",
      "    |         **model_kwargs,\n",
      "    |     )\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/generation/utils.py\", line 3614, in _sample\n",
      "    |     outputs = self(**model_inputs, return_dict=True)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    |     return self._call_impl(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    |     return forward_call(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/utils/generic.py\", line 961, in wrapper\n",
      "    |     output = func(self, *args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py\", line 460, in forward\n",
      "    |     outputs: BaseModelOutputWithPast = self.model(\n",
      "    |                                        ~~~~~~~~~~^\n",
      "    |         input_ids=input_ids,\n",
      "    |         ^^^^^^^^^^^^^^^^^^^^\n",
      "    |     ...<6 lines>...\n",
      "    |         **kwargs,\n",
      "    |         ^^^^^^^^^\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    |     return self._call_impl(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    |     return forward_call(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/utils/generic.py\", line 1069, in wrapper\n",
      "    |     outputs = func(self, *args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py\", line 390, in forward\n",
      "    |     hidden_states = decoder_layer(\n",
      "    |         hidden_states,\n",
      "    |     ...<5 lines>...\n",
      "    |         **kwargs,\n",
      "    |     )\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/modeling_layers.py\", line 94, in __call__\n",
      "    |     return super().__call__(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    |     return self._call_impl(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    |     return forward_call(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py\", line 289, in forward\n",
      "    |     hidden_states, _ = self.self_attn(\n",
      "    |                        ~~~~~~~~~~~~~~^\n",
      "    |         hidden_states=hidden_states,\n",
      "    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |     ...<6 lines>...\n",
      "    |         **kwargs,\n",
      "    |         ^^^^^^^^^\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    |     return self._call_impl(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    |     return forward_call(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py\", line 248, in forward\n",
      "    |     attn_output, attn_weights = attention_interface(\n",
      "    |                                 ~~~~~~~~~~~~~~~~~~~^\n",
      "    |         self,\n",
      "    |         ^^^^^\n",
      "    |     ...<6 lines>...\n",
      "    |         **kwargs,\n",
      "    |         ^^^^^^^^^\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/integrations/sdpa_attention.py\", line 81, in sdpa_attention_forward\n",
      "    |     attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "    |         query,\n",
      "    |     ...<6 lines>...\n",
      "    |         **sdpa_kwargs,\n",
      "    |     )\n",
      "    | RuntimeError: [enforce fail at alloc_cpu.cpp:119] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 35004463232 bytes. Error code 12 (Cannot allocate memory)\n",
      "    +---------------- 2 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/home/au338890/.local/share/uv/python/cpython-3.13.3-linux-x86_64-gnu/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
      "    |     result = coro.send(None)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/_utils.py\", line 97, in _run_task\n",
      "    |     results[index] = await tsk()\n",
      "    |                      ^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 279, in _handle_case\n",
      "    |     return await _run_task_and_evaluators(task, case, report_case_name, self.evaluators)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 882, in _run_task_and_evaluators\n",
      "    |     scoring_context = await _run_task(task, case)\n",
      "    |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 820, in _run_task\n",
      "    |     task_output = await task(case.inputs)\n",
      "    |                   ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/tmp/ipykernel_10327/1812324380.py\", line 2, in answer_question\n",
      "    |     r = agent_evaluated.run(question)\n",
      "    |   File \"/tmp/ipykernel_10327/3741609218.py\", line 25, in run\n",
      "    |     model_output = self.model.generate(**model_input, max_new_tokens=500)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    |     return func(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/generation/utils.py\", line 2633, in generate\n",
      "    |     result = self._sample(\n",
      "    |         input_ids,\n",
      "    |     ...<5 lines>...\n",
      "    |         **model_kwargs,\n",
      "    |     )\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/generation/utils.py\", line 3614, in _sample\n",
      "    |     outputs = self(**model_inputs, return_dict=True)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    |     return self._call_impl(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    |     return forward_call(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/utils/generic.py\", line 961, in wrapper\n",
      "    |     output = func(self, *args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py\", line 460, in forward\n",
      "    |     outputs: BaseModelOutputWithPast = self.model(\n",
      "    |                                        ~~~~~~~~~~^\n",
      "    |         input_ids=input_ids,\n",
      "    |         ^^^^^^^^^^^^^^^^^^^^\n",
      "    |     ...<6 lines>...\n",
      "    |         **kwargs,\n",
      "    |         ^^^^^^^^^\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    |     return self._call_impl(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    |     return forward_call(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/utils/generic.py\", line 1069, in wrapper\n",
      "    |     outputs = func(self, *args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py\", line 390, in forward\n",
      "    |     hidden_states = decoder_layer(\n",
      "    |         hidden_states,\n",
      "    |     ...<5 lines>...\n",
      "    |         **kwargs,\n",
      "    |     )\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/modeling_layers.py\", line 94, in __call__\n",
      "    |     return super().__call__(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    |     return self._call_impl(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    |     return forward_call(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py\", line 289, in forward\n",
      "    |     hidden_states, _ = self.self_attn(\n",
      "    |                        ~~~~~~~~~~~~~~^\n",
      "    |         hidden_states=hidden_states,\n",
      "    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |     ...<6 lines>...\n",
      "    |         **kwargs,\n",
      "    |         ^^^^^^^^^\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    |     return self._call_impl(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    |     return forward_call(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py\", line 248, in forward\n",
      "    |     attn_output, attn_weights = attention_interface(\n",
      "    |                                 ~~~~~~~~~~~~~~~~~~~^\n",
      "    |         self,\n",
      "    |         ^^^^^\n",
      "    |     ...<6 lines>...\n",
      "    |         **kwargs,\n",
      "    |         ^^^^^^^^^\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/integrations/sdpa_attention.py\", line 81, in sdpa_attention_forward\n",
      "    |     attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "    |         query,\n",
      "    |     ...<6 lines>...\n",
      "    |         **sdpa_kwargs,\n",
      "    |     )\n",
      "    | RuntimeError: [enforce fail at alloc_cpu.cpp:119] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 35148549248 bytes. Error code 12 (Cannot allocate memory)\n",
      "    +------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "async def answer_question(question: str) -> str:\n",
    "    r = agent_evaluated.run(question)\n",
    "    return r\n",
    "\n",
    "report_2 = ds.evaluate_sync(answer_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ff9ec58",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'report_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mreport_2\u001b[49m.print(include_input=\u001b[38;5;28;01mTrue\u001b[39;00m, include_output=\u001b[38;5;28;01mTrue\u001b[39;00m, include_expected_output=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'report_2' is not defined"
     ]
    }
   ],
   "source": [
    "report_2.print(include_input=True, include_output=True, include_expected_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f208c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4517bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hvis hver pirat svarede sandt til den givne opgave, skal et antal mønter, der er tilfældigt resulterer i et antal mønter, der er lik 30.\n",
      "I et ufuldstændigt skattecontainer, skal du vurdere, hvor mange mønter, der er i containeren, der er i den rigtige mønter.\n",
      "Antallet af mønter, der er i den rigtige mønter, er 10.\n",
      "\n",
      "Før du tager beslutningen, skal du udføre en fejlfindende analyse.\n",
      "Tælle alle mønterne i containeren, der er i den rigtige mønter.\n",
      "Antallet af mønter, der er i den rigtige mønter, er 10.\n",
      "Der er 6 mønter, der er i den rigtige mønter.\n",
      "Antallet af mønter, der er i den rigtige mønter, er 10.\n",
      "Antallet af mønter, der er i den rigtige mønter, er 10.\n",
      "\n",
      "Det er ikke sandt, at der er 10 mønter i den rigtige mønter.\n",
      "Antallet af mønter i den rigtige mønter er 10.\n",
      "For at bestemme, hvor mange mønter, der er i den rigtige mønter, er det mest almindelige.\n",
      "Tæll alle mønterne, der er i containeren, der er i den rigtige mønter.\n",
      "|     | Gold | Silver | Bronze |\n",
      "| Tom |   9   |   11   |   12   |\n",
      "| Al  |  7   |   10   |   10   |\n",
      "| Pit |  10  |   10   |   10   |\n",
      "| Jim |  9   |   10   |   10   |\n",
      "\n",
      "|     | Gold | Silver | Bronze |\n",
      "| Tom |   9   |   11   |   12   |\n",
      "| Al  |  7   |   10   |   10   |\n",
      "| Pit |  10  |   10   |   10   \n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b922864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output does not provide the correct solution to the problem. It repeats incorrect numbers for the coins and does not solve the logic puzzle based on the condition given (only one pirate tells the truth).\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0].assertion_reason)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m-gsm-symbolic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
