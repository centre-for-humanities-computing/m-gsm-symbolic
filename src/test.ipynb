{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d442606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 09:50:40 [__init__.py:235] Automatically detected platform cuda.\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import argparse\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "#import outlines\n",
    "#from outlines.types.dsl import Regex, String, at_most, either, to_regex\n",
    "#from outlines.types import zero_or_more, one_or_more, optional, whitespace, digit\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from pydantic_evals import Dataset\n",
    "from pydantic_evals.evaluators import Evaluator, EvaluatorContext\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.sampling_params import GuidedDecodingParams\n",
    "\n",
    "from m_gsm_symbolic.kaenguruen.load_data import load_kaenguruen\n",
    "from m_gsm_symbolic.load_data import load_gsm_dan, load_gsm_eng\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ab570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c58e0985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "#load_dotenv()\n",
    "#api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "from huggingface_hub import login\n",
    "#login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5dc05c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#judge_llm_name = \"openai:gpt-4o-2024-08-06\"\n",
    "response_model_name = \"meta-llama/Llama-3.2-1B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee20a52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Outlines regex DSL: reasoning (any text) + '####' + answer (int)\n",
    "#answer_pattern = to_regex(\n",
    "#    Regex(pattern=r\".\").at_most(500) +\n",
    "#    String(\"#### \") +\n",
    "#    digit.one_or_more() +\n",
    "#    optional(\n",
    "#        either(\n",
    "#            String(\".\"), String(\",\")\n",
    "#        ) + optional(\n",
    "#            digit.one_or_more()\n",
    "#        )\n",
    "#    )\n",
    "#)\n",
    "answer_pattern = r\".{,300}####\\s\\d+\"\n",
    "# Support float or frac as well\n",
    "    #sequence(\n",
    "    #    one_or_more(digit()),\n",
    "    #    optional(either(\n",
    "    #        sequence(literal(\".\"), one_or_more(digit())),\n",
    "    #        sequence(literal(\"/\"), one_or_more(digit()))\n",
    "    #    ))\n",
    "    #)\n",
    "\n",
    "class HuggingFaceAgent:\n",
    "    def __init__(self, model: str, examples: list, \n",
    "                 answer_pattern: str = answer_pattern):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        #self.model = outlines.from_transformers(AutoModelForCausalLM.from_pretrained(model, torch_dtype=\"auto\").to(device).eval(), self.tokenizer)\n",
    "        \n",
    "        #self.model = AutoModelForCausalLM.from_pretrained(model, torch_dtype=\"auto\").to(device)\n",
    "        self.model = LLM(model=model)\n",
    "        self.cases = examples\n",
    "        self.answer_pattern = answer_pattern\n",
    "\n",
    "    def _build_prompt(self, prompt):\n",
    "        examples = []\n",
    "        for case in self.cases:\n",
    "            example = f\"Problem: {case.inputs}\\n\\nSolution: {case.expected_output}\"\n",
    "            examples.append(example)\n",
    "\n",
    "        prompt = f\"Problem: {prompt}\\n\\nSolution:\"\n",
    "        examples.append(prompt)\n",
    "        prompt = \"\\n\\n\".join(examples)\n",
    "        return prompt\n",
    "\n",
    "    def run(self, prompt: str):\n",
    "        prompt = self._build_prompt(prompt)\n",
    "        guided_params = GuidedDecodingParams(regex=self.answer_pattern)\n",
    "        sampling_params = SamplingParams(guided_decoding=guided_params)\n",
    "        model_output = self.model.generate(\n",
    "            prompt,\n",
    "            sampling_params=sampling_params\n",
    "        )\n",
    "        #model_output = self.model(prompt, answer_pattern, max_new_tokens=500)\n",
    "        #inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        #inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        #model_output = self.model.generate(**inputs, max_new_tokens=500)\n",
    "        #model_output = self.tokenizer.decode(model_output[0], skip_special_tokens=True)\n",
    "        return model_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0143d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 0: 40\n",
      "Case 1: 58\n",
      "Case 2: 30\n",
      "Case 3: 38\n",
      "Case 4: 56\n",
      "Case 5: 4\n",
      "Case 6: 18\n",
      "Case 7: 54\n",
      "Case 8: 210\n",
      "Case 9: 1050\n",
      "Case 10: 8\n",
      "Case 11: 4\n",
      "Case 12: 5\n",
      "Case 13: 55\n",
      "Case 14: 2\n",
      "Case 15: 11\n",
      "Case 16: 77\n",
      "Case 17: 118000\n",
      "Case 18: 16\n",
      "Case 19: 540\n",
      "Case 20: 35\n",
      "Case 21: 43200\n",
      "Case 22: 64\n",
      "Case 23: 12\n",
      "Case 24: 88\n",
      "Case 25: 10\n",
      "Case 26: 130\n",
      "Case 27: 342\n",
      "Case 28: 70\n",
      "Case 29: 320\n",
      "Case 30: 157\n",
      "Case 31: 2\n",
      "Case 32: 25\n",
      "Case 33: 72\n",
      "Case 34: 75\n",
      "Case 35: 140\n",
      "Case 36: 16\n",
      "Case 37: 14400\n",
      "Case 38: 2\n",
      "Case 39: 3200\n",
      "Case 40: 100\n",
      "Case 41: 5\n",
      "Case 42: 14\n",
      "Case 43: 4\n",
      "Case 44: 63\n",
      "Case 45: 2\n",
      "Case 46: 15\n",
      "Case 47: 200\n",
      "Case 48: 4\n",
      "Case 49: 694\n",
      "Case 50: 4000\n",
      "Case 51: 6\n",
      "Case 52: 9\n",
      "Case 53: 159\n",
      "Case 54: 48\n",
      "Case 55: 45\n",
      "Case 56: 100\n",
      "Case 57: 3\n",
      "Case 58: 42\n",
      "Case 59: 48\n",
      "Case 60: 428\n",
      "Case 61: 25\n",
      "Case 62: 75\n",
      "Case 63: 10\n",
      "Case 64: 3430\n",
      "Case 65: 18\n",
      "Case 66: 78\n",
      "Case 67: 16\n",
      "Case 68: 8\n",
      "Case 69: 49\n",
      "Case 70: 3\n",
      "Case 71: 20\n",
      "Case 72: 34\n",
      "Case 73: 36\n",
      "Case 74: 48\n",
      "Case 75: 1248\n",
      "Case 76: 9500\n",
      "Case 77: 16\n",
      "Case 78: 2\n",
      "Case 79: 36\n",
      "Case 80: 98\n",
      "Case 81: 30\n",
      "Case 82: 44\n",
      "Case 83: 30\n",
      "Case 84: 7\n",
      "Case 85: 78\n",
      "Case 86: 2\n",
      "Case 87: 19\n",
      "Case 88: 2325\n",
      "Case 89: 13\n",
      "Case 90: 750\n",
      "Case 91: 12\n",
      "Case 92: 22\n",
      "Case 93: 20\n",
      "Case 94: 15\n",
      "Case 95: 20\n",
      "Case 96: 10\n",
      "Case 97: 50\n",
      "Case 98: 11\n",
      "Case 99: 50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cases = [p.to_case() for p in load_gsm_eng()]\n",
    "\n",
    "for i, case in enumerate(cases):\n",
    "    print(f\"Case {i}: {re.search(r'####\\s*((\\d|,)+)', case.expected_output).group(1).replace(',', '')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8729d50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = [p.to_case() for p in load_gsm_eng()]\n",
    "\n",
    "response_model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "agent_evaluated = HuggingFaceAgent(response_model_name, examples=cases[-3:], answer_pattern=answer_pattern)\n",
    "\n",
    "# Custom evaluator: compare only the answer after '####'\n",
    "@dataclass\n",
    "class AnswerOnlyMatch(Evaluator):\n",
    "    def evaluate(self, ctx: EvaluatorContext) -> bool:\n",
    "        # Extract answer after '####' using regex\n",
    "        m_pred = re.search(r\"####\\s*((\\d|,)+)\", ctx.output)\n",
    "        m_true = re.search(r\"####\\s*((\\d|,)+)\", ctx.expected_output)\n",
    "        if not m_pred or not m_true:\n",
    "            return False\n",
    "        return float(m_pred.group(1).replace(\",\",\"\")) == float(m_true.group(1).replace(\",\",\"\"))\n",
    "\n",
    "ds = Dataset(\n",
    "    cases=cases[:2],\n",
    "    evaluators=[AnswerOnlyMatch()],\n",
    ")\n",
    "\n",
    "async def answer_question(question: str) -> str:\n",
    "    r = agent_evaluated.run(question)\n",
    "    return r\n",
    "\n",
    "report = ds.evaluate_sync(answer_question)\n",
    "report.print(include_input=True, include_output=True, include_expected_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "209d5cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 09:51:00 [config.py:3392] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.\n",
      "WARNING 08-11 09:51:00 [config.py:3443] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 08-11 09:51:00 [config.py:1604] Using max model len 131072\n",
      "WARNING 08-11 09:51:00 [arg_utils.py:1690] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "WARNING 08-11 09:51:00 [arg_utils.py:1486] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.\n",
      "INFO 08-11 09:51:03 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 08-11 09:51:03 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 08-11 09:51:06 [cuda.py:346] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 08-11 09:51:06 [cuda.py:395] Using XFormers backend.\n",
      "INFO 08-11 09:51:07 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 08-11 09:51:07 [model_runner.py:1083] Starting to load model meta-llama/Llama-3.2-1B...\n",
      "INFO 08-11 09:51:08 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "INFO 08-11 09:51:08 [weight_utils.py:349] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21a0209795e4ce6a9741d13ac09efd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 09:51:09 [default_loader.py:262] Loading weights took 1.01 seconds\n",
      "INFO 08-11 09:51:10 [model_runner.py:1115] Model loading took 2.3185 GiB and 1.678835 seconds\n",
      "INFO 08-11 09:51:11 [worker.py:295] Memory profiling takes 0.70 seconds\n",
      "INFO 08-11 09:51:11 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.27GiB) x gpu_memory_utilization (0.90) = 42.54GiB\n",
      "INFO 08-11 09:51:11 [worker.py:295] model weights take 2.32GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 38.99GiB.\n",
      "INFO 08-11 09:51:11 [executor_base.py:113] # cuda blocks: 79843, # CPU blocks: 8192\n",
      "INFO 08-11 09:51:11 [executor_base.py:118] Maximum concurrency for 131072 tokens per request: 9.75x\n",
      "INFO 08-11 09:51:13 [model_runner.py:1385] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f10803042104b1f99027b6dd96b3d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 09:51:39 [model_runner.py:1537] Graph capturing finished in 25 secs, took 0.13 GiB\n",
      "INFO 08-11 09:51:39 [llm_engine.py:424] init engine (profile, create kv cache, warmup model) took 29.09 seconds\n",
      "Case:\n",
      "Case(name='661', inputs=\"Roger goes to the store to buy some coffee. The normal brand of coffee he buys costs $5 per pound. He had to buy a more expensive brand that costs 20% more since his favorite brand was sold out. He decides to buy a week's worth of coffee and he uses 1 pound of coffee per day. He also decided to buy himself a donut for $2. How much did everything cost?\", metadata={'filepath': '/home/simonenni/repos/m-gsm-symbolic/data/templates/eng/symbolic/0042.json'}, expected_output='The coffee he bought was 5*0.2=$<<5*0.2=1>>1 more expensive per pound than what he normally buys\\nSo it cost 5+1=$<<5+1=6>>6 per pound\\nHe goes through 1*7=<<1*7=7>>7 pounds of coffee a week\\nSo he paid 6*7=$<<6*7=42>>42 on coffee\\nThat means his total bill was 42+2=$<<42+2=44>>44\\n#### 44', evaluators=[])\n",
      "Output:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c8f78d27024499a468a14600684877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd47a2f3e9a482eb172ce073b01b437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported conversion from f16 to f16\n",
      "LLVM ERROR: Unsupported rounding mode for conversion.\n",
      "#blocked = #ttg.blocked<{sizePerThread = [4, 4], threadsPerWarp = [8, 4], warpsPerCTA = [4, 1], order = [1, 0]}>\n",
      "#blocked1 = #ttg.blocked<{sizePerThread = [4, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>\n",
      "#blocked2 = #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n",
      "#blocked3 = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n",
      "#blocked4 = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [0, 1]}>\n",
      "#shared = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n",
      "#shared1 = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [0, 1]}>\n",
      "#smem = #ttg.shared_memory\n",
      "module attributes {\"ttg.num-ctas\" = 1 : i32, \"ttg.num-warps\" = 4 : i32, ttg.target = \"cuda:75\", \"ttg.threads-per-warp\" = 32 : i32} {\n",
      "  tt.func public @_fwd_kernel(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg5: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg6: f32, %arg7: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg8: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg9: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg10: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg11: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg12: i32, %arg13: i32 {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32 {tt.divisibility = 16 : i32}, %arg16: i32 {tt.divisibility = 16 : i32}, %arg17: i32 {tt.divisibility = 16 : i32}, %arg18: i32 {tt.divisibility = 16 : i32}, %arg19: i32 {tt.divisibility = 16 : i32}, %arg20: i32 {tt.divisibility = 16 : i32}, %arg21: i32 {tt.divisibility = 16 : i32}, %arg22: i32 {tt.divisibility = 16 : i32}, %arg23: i32 {tt.divisibility = 16 : i32}, %arg24: i32 {tt.divisibility = 16 : i32}, %arg25: i32 {tt.divisibility = 16 : i32}, %arg26: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {\n",
      "    %cst = arith.constant dense<0xFF800000> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "    %cst_0 = arith.constant dense<1.000000e+00> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "    %cst_1 = arith.constant dense<0xFF800000> : tensor<128x16xf32, #blocked>\n",
      "    %cst_2 = arith.constant dense<0xFF800000> : tensor<128x64xf32, #blocked1>\n",
      "    %cst_3 = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #blocked1>\n",
      "    %c64_i32 = arith.constant 64 : i32\n",
      "    %c0_i32 = arith.constant 0 : i32\n",
      "    %c16_i32 = arith.constant 16 : i32\n",
      "    %cst_4 = arith.constant dense<0.000000e+00> : tensor<64x64xf16, #blocked2>\n",
      "    %cst_5 = arith.constant dense<0.000000e+00> : tensor<64x64xf16, #blocked3>\n",
      "    %c48_i32 = arith.constant 48 : i32\n",
      "    %c32_i32 = arith.constant 32 : i32\n",
      "    %cst_6 = arith.constant dense<0.000000e+00> : tensor<16x64xf16, #blocked4>\n",
      "    %cst_7 = arith.constant dense<0.000000e+00> : tensor<128x16xf32, #blocked>\n",
      "    %cst_8 = arith.constant dense<0.000000e+00> : tensor<64x16xf16, #blocked3>\n",
      "    %c15_i32 = arith.constant 15 : i32\n",
      "    %cst_9 = arith.constant dense<0.000000e+00> : tensor<128x64xf16, #blocked2>\n",
      "    %c4_i32 = arith.constant 4 : i32\n",
      "    %c1_i32 = arith.constant 1 : i32\n",
      "    %c128_i32 = arith.constant 128 : i32\n",
      "    %cst_10 = arith.constant dense<64> : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked2}>>\n",
      "    %cst_11 = arith.constant dense<64> : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked4}>>\n",
      "    %cst_12 = arith.constant dense<64> : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>\n",
      "    %cst_13 = arith.constant dense<1> : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked2}>>\n",
      "    %cst_14 = arith.constant dense<1> : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked4}>>\n",
      "    %cst_15 = arith.constant dense<1> : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>\n",
      "    %cst_16 = arith.constant dense<0> : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked2}>>\n",
      "    %cst_17 = arith.constant dense<0> : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked4}>>\n",
      "    %cst_18 = arith.constant dense<0> : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>\n",
      "    %cst_19 = arith.constant dense<8> : tensor<64x1xi32, #blocked3>\n",
      "    %cst_20 = arith.constant dense<16> : tensor<1x16xi32, #blocked3>\n",
      "    %cst_21 = arith.constant dense<8> : tensor<1x16xi32, #blocked3>\n",
      "    %0 = tt.get_program_id x : i32\n",
      "    %1 = tt.get_program_id y : i32\n",
      "    %2 = tt.get_program_id z : i32\n",
      "    %3 = arith.divsi %1, %c4_i32 : i32\n",
      "    %4 = tt.addptr %arg10, %0 : !tt.ptr<i32>, i32\n",
      "    %5 = tt.load %4 : !tt.ptr<i32>\n",
      "    %6 = tt.addptr %arg9, %0 : !tt.ptr<i32>, i32\n",
      "    %7 = tt.load %6 : !tt.ptr<i32>\n",
      "    %8 = tt.addptr %6, %c1_i32 : !tt.ptr<i32>, i32\n",
      "    %9 = tt.load %8 : !tt.ptr<i32>\n",
      "    %10 = arith.subi %9, %7 : i32\n",
      "    %11 = arith.subi %5, %10 : i32\n",
      "    %12 = arith.muli %2, %c128_i32 : i32\n",
      "    %13 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked2}>>\n",
      "    %14 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked4}>>\n",
      "    %15 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>\n",
      "    %16 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>\n",
      "    %17 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>\n",
      "    %18 = tt.splat %12 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>\n",
      "    %19 = tt.splat %12 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>\n",
      "    %20 = arith.addi %18, %16 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>\n",
      "    %21 = arith.addi %19, %17 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>\n",
      "    %22 = tt.expand_dims %20 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<128x1xi32, #blocked2>\n",
      "    %23 = tt.expand_dims %21 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<128x1xi32, #blocked1>\n",
      "    %24 = tt.splat %7 : i32 -> tensor<128x1xi32, #blocked2>\n",
      "    %25 = arith.addi %24, %22 : tensor<128x1xi32, #blocked2>\n",
      "    %26 = tt.splat %arg13 : i32 -> tensor<128x1xi32, #blocked2>\n",
      "    %27 = arith.muli %25, %26 : tensor<128x1xi32, #blocked2>\n",
      "    %28 = arith.muli %1, %arg14 : i32\n",
      "    %29 = tt.splat %28 : i32 -> tensor<128x1xi32, #blocked2>\n",
      "    %30 = arith.addi %27, %29 : tensor<128x1xi32, #blocked2>\n",
      "    %31 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>\n",
      "    %32 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked1}>>\n",
      "    %33 = tt.expand_dims %13 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked2}>> -> tensor<1x64xi32, #blocked2>\n",
      "    %34 = tt.expand_dims %14 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x64xi32, #blocked4>\n",
      "    %35 = tt.expand_dims %31 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x64xi32, #blocked3>\n",
      "    %36 = tt.expand_dims %32 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x64xi32, #blocked1>\n",
      "    %37 = tt.broadcast %30 : tensor<128x1xi32, #blocked2> -> tensor<128x64xi32, #blocked2>\n",
      "    %38 = tt.broadcast %33 : tensor<1x64xi32, #blocked2> -> tensor<128x64xi32, #blocked2>\n",
      "    %39 = arith.addi %37, %38 : tensor<128x64xi32, #blocked2>\n",
      "    %40 = arith.cmpi slt, %13, %cst_10 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked2}>>\n",
      "    %41 = arith.cmpi slt, %14, %cst_11 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked4}>>\n",
      "    %42 = arith.cmpi slt, %15, %cst_12 : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>\n",
      "    %43 = arith.select %40, %cst_13, %cst_16 : tensor<64xi1, #ttg.slice<{dim = 0, parent = #blocked2}>>, tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked2}>>\n",
      "    %44 = arith.select %41, %cst_14, %cst_17 : tensor<64xi1, #ttg.slice<{dim = 0, parent = #blocked4}>>, tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked4}>>\n",
      "    %45 = arith.select %42, %cst_15, %cst_18 : tensor<64xi1, #ttg.slice<{dim = 1, parent = #blocked3}>>, tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>\n",
      "    %46 = arith.cmpi ne, %43, %cst_16 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked2}>>\n",
      "    %47 = arith.cmpi ne, %44, %cst_17 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked4}>>\n",
      "    %48 = arith.cmpi ne, %45, %cst_18 : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>\n",
      "    %49 = tt.expand_dims %46 {axis = 0 : i32} : tensor<64xi1, #ttg.slice<{dim = 0, parent = #blocked2}>> -> tensor<1x64xi1, #blocked2>\n",
      "    %50 = tt.expand_dims %47 {axis = 0 : i32} : tensor<64xi1, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x64xi1, #blocked4>\n",
      "    %51 = tt.splat %10 : i32 -> tensor<128x1xi32, #blocked2>\n",
      "    %52 = arith.cmpi slt, %22, %51 : tensor<128x1xi32, #blocked2>\n",
      "    %53 = tt.broadcast %49 : tensor<1x64xi1, #blocked2> -> tensor<128x64xi1, #blocked2>\n",
      "    %54 = tt.broadcast %52 : tensor<128x1xi1, #blocked2> -> tensor<128x64xi1, #blocked2>\n",
      "    %55 = arith.andi %53, %54 : tensor<128x64xi1, #blocked2>\n",
      "    %56 = tt.splat %arg0 : !tt.ptr<f16> -> tensor<128x64x!tt.ptr<f16>, #blocked2>\n",
      "    %57 = tt.addptr %56, %39 : tensor<128x64x!tt.ptr<f16>, #blocked2>, tensor<128x64xi32, #blocked2>\n",
      "    %58 = tt.load %57, %55, %cst_9 : tensor<128x64x!tt.ptr<f16>, #blocked2>\n",
      "    %59 = arith.addi %11, %c15_i32 : i32\n",
      "    %60 = arith.divui %59, %c16_i32 : i32\n",
      "    %61 = arith.remsi %60, %c4_i32 : i32\n",
      "    %62 = arith.subi %60, %61 : i32\n",
      "    %63 = arith.muli %62, %c16_i32 : i32\n",
      "    %64 = arith.muli %0, %arg12 : i32\n",
      "    %65 = tt.addptr %arg5, %64 : !tt.ptr<i32>, i32\n",
      "    %66 = arith.muli %3, %arg22 : i32\n",
      "    %67 = tt.expand_dims %15 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<64x1xi32, #blocked3>\n",
      "    %68 = arith.divsi %67, %cst_19 : tensor<64x1xi32, #blocked3>\n",
      "    %69 = tt.splat %arg23 : i32 -> tensor<64x1xi32, #blocked3>\n",
      "    %70 = arith.muli %68, %69 : tensor<64x1xi32, #blocked3>\n",
      "    %71 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>\n",
      "    %72 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked}>>\n",
      "    %73 = tt.expand_dims %71 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x16xi32, #blocked3>\n",
      "    %74 = tt.expand_dims %72 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked}>> -> tensor<1x16xi32, #blocked>\n",
      "    %75 = arith.remsi %67, %cst_19 : tensor<64x1xi32, #blocked3>\n",
      "    %76 = tt.broadcast %75 : tensor<64x1xi32, #blocked3> -> tensor<64x16xi32, #blocked3>\n",
      "    %77 = arith.muli %3, %arg25 : i32\n",
      "    %78 = tt.splat %arg26 : i32 -> tensor<1x64xi32, #blocked4>\n",
      "    %79 = arith.muli %34, %78 : tensor<1x64xi32, #blocked4>\n",
      "    %80 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>\n",
      "    %81 = tt.expand_dims %80 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>\n",
      "    %82 = tt.broadcast %81 : tensor<16x1xi32, #blocked4> -> tensor<16x64xi32, #blocked4>\n",
      "    %83 = tt.fp_to_fp %58 : tensor<128x64xf16, #blocked2> -> tensor<128x64xf32, #blocked2>\n",
      "    %84 = ttg.local_alloc %83 : (tensor<128x64xf32, #blocked2>) -> !ttg.memdesc<128x64xf32, #shared, #smem>\n",
      "    %85 = tt.splat %11 : i32 -> tensor<1x16xi32, #blocked>\n",
      "    %86 = tt.splat %arg6 : f32 -> tensor<128x16xf32, #blocked>\n",
      "    %87:3 = scf.for %arg27 = %c0_i32 to %63 step %c64_i32 iter_args(%arg28 = %cst_3, %arg29 = %cst_0, %arg30 = %cst) -> (tensor<128x64xf32, #blocked1>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>)  : i32 {\n",
      "      %140 = arith.divsi %arg27, %c16_i32 : i32\n",
      "      %141 = tt.addptr %65, %140 : !tt.ptr<i32>, i32\n",
      "      %142 = tt.load %141 : !tt.ptr<i32>\n",
      "      %143 = arith.muli %142, %arg21 : i32\n",
      "      %144 = arith.addi %143, %66 : i32\n",
      "      %145 = tt.splat %144 : i32 -> tensor<64x1xi32, #blocked3>\n",
      "      %146 = arith.addi %145, %70 : tensor<64x1xi32, #blocked3>\n",
      "      %147 = tt.splat %arg27 : i32 -> tensor<1x16xi32, #blocked3>\n",
      "      %148 = tt.splat %arg27 : i32 -> tensor<1x16xi32, #blocked>\n",
      "      %149 = arith.addi %147, %73 : tensor<1x16xi32, #blocked3>\n",
      "      %150 = arith.addi %148, %74 : tensor<1x16xi32, #blocked>\n",
      "      %151 = arith.remsi %149, %cst_20 : tensor<1x16xi32, #blocked3>\n",
      "      %152 = arith.muli %151, %cst_21 : tensor<1x16xi32, #blocked3>\n",
      "      %153 = tt.broadcast %146 : tensor<64x1xi32, #blocked3> -> tensor<64x16xi32, #blocked3>\n",
      "      %154 = tt.broadcast %152 : tensor<1x16xi32, #blocked3> -> tensor<64x16xi32, #blocked3>\n",
      "      %155 = arith.addi %153, %154 : tensor<64x16xi32, #blocked3>\n",
      "      %156 = arith.addi %155, %76 : tensor<64x16xi32, #blocked3>\n",
      "      %157 = arith.muli %142, %arg24 : i32\n",
      "      %158 = arith.addi %157, %77 : i32\n",
      "      %159 = tt.splat %158 : i32 -> tensor<1x64xi32, #blocked4>\n",
      "      %160 = arith.addi %159, %79 : tensor<1x64xi32, #blocked4>\n",
      "      %161 = tt.broadcast %160 : tensor<1x64xi32, #blocked4> -> tensor<16x64xi32, #blocked4>\n",
      "      %162 = arith.addi %161, %82 : tensor<16x64xi32, #blocked4>\n",
      "      %163 = arith.addi %arg27, %c16_i32 : i32\n",
      "      %164 = arith.cmpi sgt, %163, %11 : i32\n",
      "      %165 = scf.if %164 -> (tensor<64x16xf16, #blocked3>) {\n",
      "        %373 = tt.expand_dims %48 {axis = 1 : i32} : tensor<64xi1, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<64x1xi1, #blocked3>\n",
      "        %374 = tt.splat %11 : i32 -> tensor<1x16xi32, #blocked3>\n",
      "        %375 = arith.cmpi slt, %149, %374 : tensor<1x16xi32, #blocked3>\n",
      "        %376 = tt.broadcast %373 : tensor<64x1xi1, #blocked3> -> tensor<64x16xi1, #blocked3>\n",
      "        %377 = tt.broadcast %375 : tensor<1x16xi1, #blocked3> -> tensor<64x16xi1, #blocked3>\n",
      "        %378 = arith.andi %376, %377 : tensor<64x16xi1, #blocked3>\n",
      "        %379 = tt.splat %arg3 : !tt.ptr<f16> -> tensor<64x16x!tt.ptr<f16>, #blocked3>\n",
      "        %380 = tt.addptr %379, %156 : tensor<64x16x!tt.ptr<f16>, #blocked3>, tensor<64x16xi32, #blocked3>\n",
      "        %381 = tt.load %380, %378, %cst_8 : tensor<64x16x!tt.ptr<f16>, #blocked3>\n",
      "        scf.yield %381 : tensor<64x16xf16, #blocked3>\n",
      "      } else {\n",
      "        %373 = tt.splat %arg3 : !tt.ptr<f16> -> tensor<64x16x!tt.ptr<f16>, #blocked3>\n",
      "        %374 = tt.addptr %373, %156 : tensor<64x16x!tt.ptr<f16>, #blocked3>, tensor<64x16xi32, #blocked3>\n",
      "        %375 = tt.load %374 : tensor<64x16x!tt.ptr<f16>, #blocked3>\n",
      "        scf.yield %375 : tensor<64x16xf16, #blocked3>\n",
      "      }\n",
      "      %166 = tt.fp_to_fp %165 : tensor<64x16xf16, #blocked3> -> tensor<64x16xf32, #blocked3>\n",
      "      %167 = ttg.local_alloc %166 : (tensor<64x16xf32, #blocked3>) -> !ttg.memdesc<64x16xf32, #shared1, #smem>\n",
      "      %168 = ttg.local_load %84 : !ttg.memdesc<128x64xf32, #shared, #smem> -> tensor<128x64xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked}>>\n",
      "      %169 = ttg.local_load %167 : !ttg.memdesc<64x16xf32, #shared1, #smem> -> tensor<64x16xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>\n",
      "      %170 = tt.dot %168, %169, %cst_7, inputPrecision = tf32 : tensor<128x64xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<64x16xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<128x16xf32, #blocked>\n",
      "      %171 = arith.cmpi slt, %150, %85 : tensor<1x16xi32, #blocked>\n",
      "      %172 = tt.broadcast %171 : tensor<1x16xi1, #blocked> -> tensor<128x16xi1, #blocked>\n",
      "      %173 = arith.select %172, %170, %cst_1 : tensor<128x16xi1, #blocked>, tensor<128x16xf32, #blocked>\n",
      "      %174 = arith.mulf %173, %86 : tensor<128x16xf32, #blocked>\n",
      "      %175 = \"tt.reduce\"(%174) <{axis = 1 : i32}> ({\n",
      "      ^bb0(%arg31: f32, %arg32: f32):\n",
      "        %373 = arith.maxnumf %arg31, %arg32 : f32\n",
      "        tt.reduce.return %373 : f32\n",
      "      }) : (tensor<128x16xf32, #blocked>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %176 = arith.maxnumf %arg30, %175 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %177 = tt.expand_dims %176 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked>\n",
      "      %178 = tt.broadcast %177 : tensor<128x1xf32, #blocked> -> tensor<128x16xf32, #blocked>\n",
      "      %179 = arith.subf %174, %178 : tensor<128x16xf32, #blocked>\n",
      "      %180 = math.exp %179 : tensor<128x16xf32, #blocked>\n",
      "      %181 = \"tt.reduce\"(%180) <{axis = 1 : i32}> ({\n",
      "      ^bb0(%arg31: f32, %arg32: f32):\n",
      "        %373 = arith.addf %arg31, %arg32 : f32\n",
      "        tt.reduce.return %373 : f32\n",
      "      }) : (tensor<128x16xf32, #blocked>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %182 = arith.subf %arg30, %176 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %183 = math.exp %182 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %184 = tt.expand_dims %183 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked>\n",
      "      %185 = ttg.convert_layout %184 : tensor<128x1xf32, #blocked> -> tensor<128x1xf32, #blocked1>\n",
      "      %186 = tt.broadcast %185 : tensor<128x1xf32, #blocked1> -> tensor<128x64xf32, #blocked1>\n",
      "      %187 = arith.mulf %arg28, %186 : tensor<128x64xf32, #blocked1>\n",
      "      %188 = scf.if %164 -> (tensor<16x64xf16, #blocked4>) {\n",
      "        %373 = tt.splat %arg27 : i32 -> tensor<16x1xi32, #blocked4>\n",
      "        %374 = arith.addi %373, %81 : tensor<16x1xi32, #blocked4>\n",
      "        %375 = tt.splat %11 : i32 -> tensor<16x1xi32, #blocked4>\n",
      "        %376 = arith.cmpi slt, %374, %375 : tensor<16x1xi32, #blocked4>\n",
      "        %377 = tt.broadcast %50 : tensor<1x64xi1, #blocked4> -> tensor<16x64xi1, #blocked4>\n",
      "        %378 = tt.broadcast %376 : tensor<16x1xi1, #blocked4> -> tensor<16x64xi1, #blocked4>\n",
      "        %379 = arith.andi %377, %378 : tensor<16x64xi1, #blocked4>\n",
      "        %380 = tt.splat %arg4 : !tt.ptr<f16> -> tensor<16x64x!tt.ptr<f16>, #blocked4>\n",
      "        %381 = tt.addptr %380, %162 : tensor<16x64x!tt.ptr<f16>, #blocked4>, tensor<16x64xi32, #blocked4>\n",
      "        %382 = tt.load %381, %379, %cst_6 : tensor<16x64x!tt.ptr<f16>, #blocked4>\n",
      "        scf.yield %382 : tensor<16x64xf16, #blocked4>\n",
      "      } else {\n",
      "        %373 = tt.splat %arg4 : !tt.ptr<f16> -> tensor<16x64x!tt.ptr<f16>, #blocked4>\n",
      "        %374 = tt.addptr %373, %162 : tensor<16x64x!tt.ptr<f16>, #blocked4>, tensor<16x64xi32, #blocked4>\n",
      "        %375 = tt.load %374 : tensor<16x64x!tt.ptr<f16>, #blocked4>\n",
      "        scf.yield %375 : tensor<16x64xf16, #blocked4>\n",
      "      }\n",
      "      %189 = arith.truncf %180 : tensor<128x16xf32, #blocked> to tensor<128x16xf16, #blocked>\n",
      "      %190 = tt.fp_to_fp %189 : tensor<128x16xf16, #blocked> -> tensor<128x16xf32, #blocked>\n",
      "      %191 = ttg.local_alloc %190 : (tensor<128x16xf32, #blocked>) -> !ttg.memdesc<128x16xf32, #shared, #smem>\n",
      "      %192 = ttg.local_load %191 : !ttg.memdesc<128x16xf32, #shared, #smem> -> tensor<128x16xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>>\n",
      "      %193 = tt.fp_to_fp %188 : tensor<16x64xf16, #blocked4> -> tensor<16x64xf32, #blocked4>\n",
      "      %194 = ttg.local_alloc %193 : (tensor<16x64xf32, #blocked4>) -> !ttg.memdesc<16x64xf32, #shared1, #smem>\n",
      "      %195 = ttg.local_load %194 : !ttg.memdesc<16x64xf32, #shared1, #smem> -> tensor<16x64xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>>\n",
      "      %196 = tt.dot %192, %195, %187, inputPrecision = tf32 : tensor<128x16xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>> * tensor<16x64xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>> -> tensor<128x64xf32, #blocked1>\n",
      "      %197 = arith.mulf %arg29, %183 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %198 = arith.addf %197, %181 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %199 = arith.divsi %163, %c16_i32 : i32\n",
      "      %200 = tt.addptr %65, %199 : !tt.ptr<i32>, i32\n",
      "      %201 = tt.load %200 : !tt.ptr<i32>\n",
      "      %202 = arith.muli %201, %arg21 : i32\n",
      "      %203 = arith.addi %202, %66 : i32\n",
      "      %204 = tt.splat %203 : i32 -> tensor<64x1xi32, #blocked3>\n",
      "      %205 = arith.addi %204, %70 : tensor<64x1xi32, #blocked3>\n",
      "      %206 = tt.splat %163 : i32 -> tensor<1x16xi32, #blocked3>\n",
      "      %207 = tt.splat %163 : i32 -> tensor<1x16xi32, #blocked>\n",
      "      %208 = arith.addi %206, %73 : tensor<1x16xi32, #blocked3>\n",
      "      %209 = arith.addi %207, %74 : tensor<1x16xi32, #blocked>\n",
      "      %210 = arith.remsi %208, %cst_20 : tensor<1x16xi32, #blocked3>\n",
      "      %211 = arith.muli %210, %cst_21 : tensor<1x16xi32, #blocked3>\n",
      "      %212 = tt.broadcast %205 : tensor<64x1xi32, #blocked3> -> tensor<64x16xi32, #blocked3>\n",
      "      %213 = tt.broadcast %211 : tensor<1x16xi32, #blocked3> -> tensor<64x16xi32, #blocked3>\n",
      "      %214 = arith.addi %212, %213 : tensor<64x16xi32, #blocked3>\n",
      "      %215 = arith.addi %214, %76 : tensor<64x16xi32, #blocked3>\n",
      "      %216 = arith.muli %201, %arg24 : i32\n",
      "      %217 = arith.addi %216, %77 : i32\n",
      "      %218 = tt.splat %217 : i32 -> tensor<1x64xi32, #blocked4>\n",
      "      %219 = arith.addi %218, %79 : tensor<1x64xi32, #blocked4>\n",
      "      %220 = tt.broadcast %219 : tensor<1x64xi32, #blocked4> -> tensor<16x64xi32, #blocked4>\n",
      "      %221 = arith.addi %220, %82 : tensor<16x64xi32, #blocked4>\n",
      "      %222 = arith.addi %arg27, %c32_i32 : i32\n",
      "      %223 = arith.cmpi sgt, %222, %11 : i32\n",
      "      %224 = scf.if %223 -> (tensor<64x16xf16, #blocked3>) {\n",
      "        %373 = tt.expand_dims %48 {axis = 1 : i32} : tensor<64xi1, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<64x1xi1, #blocked3>\n",
      "        %374 = tt.splat %11 : i32 -> tensor<1x16xi32, #blocked3>\n",
      "        %375 = arith.cmpi slt, %208, %374 : tensor<1x16xi32, #blocked3>\n",
      "        %376 = tt.broadcast %373 : tensor<64x1xi1, #blocked3> -> tensor<64x16xi1, #blocked3>\n",
      "        %377 = tt.broadcast %375 : tensor<1x16xi1, #blocked3> -> tensor<64x16xi1, #blocked3>\n",
      "        %378 = arith.andi %376, %377 : tensor<64x16xi1, #blocked3>\n",
      "        %379 = tt.splat %arg3 : !tt.ptr<f16> -> tensor<64x16x!tt.ptr<f16>, #blocked3>\n",
      "        %380 = tt.addptr %379, %215 : tensor<64x16x!tt.ptr<f16>, #blocked3>, tensor<64x16xi32, #blocked3>\n",
      "        %381 = tt.load %380, %378, %cst_8 : tensor<64x16x!tt.ptr<f16>, #blocked3>\n",
      "        scf.yield %381 : tensor<64x16xf16, #blocked3>\n",
      "      } else {\n",
      "        %373 = tt.splat %arg3 : !tt.ptr<f16> -> tensor<64x16x!tt.ptr<f16>, #blocked3>\n",
      "        %374 = tt.addptr %373, %215 : tensor<64x16x!tt.ptr<f16>, #blocked3>, tensor<64x16xi32, #blocked3>\n",
      "        %375 = tt.load %374 : tensor<64x16x!tt.ptr<f16>, #blocked3>\n",
      "        scf.yield %375 : tensor<64x16xf16, #blocked3>\n",
      "      }\n",
      "      %225 = tt.fp_to_fp %224 : tensor<64x16xf16, #blocked3> -> tensor<64x16xf32, #blocked3>\n",
      "      %226 = ttg.local_alloc %225 : (tensor<64x16xf32, #blocked3>) -> !ttg.memdesc<64x16xf32, #shared1, #smem>\n",
      "      %227 = ttg.local_load %226 : !ttg.memdesc<64x16xf32, #shared1, #smem> -> tensor<64x16xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>\n",
      "      %228 = tt.dot %168, %227, %cst_7, inputPrecision = tf32 : tensor<128x64xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<64x16xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<128x16xf32, #blocked>\n",
      "      %229 = arith.cmpi slt, %209, %85 : tensor<1x16xi32, #blocked>\n",
      "      %230 = tt.broadcast %229 : tensor<1x16xi1, #blocked> -> tensor<128x16xi1, #blocked>\n",
      "      %231 = arith.select %230, %228, %cst_1 : tensor<128x16xi1, #blocked>, tensor<128x16xf32, #blocked>\n",
      "      %232 = arith.mulf %231, %86 : tensor<128x16xf32, #blocked>\n",
      "      %233 = \"tt.reduce\"(%232) <{axis = 1 : i32}> ({\n",
      "      ^bb0(%arg31: f32, %arg32: f32):\n",
      "        %373 = arith.maxnumf %arg31, %arg32 : f32\n",
      "        tt.reduce.return %373 : f32\n",
      "      }) : (tensor<128x16xf32, #blocked>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %234 = arith.maxnumf %176, %233 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %235 = tt.expand_dims %234 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked>\n",
      "      %236 = tt.broadcast %235 : tensor<128x1xf32, #blocked> -> tensor<128x16xf32, #blocked>\n",
      "      %237 = arith.subf %232, %236 : tensor<128x16xf32, #blocked>\n",
      "      %238 = math.exp %237 : tensor<128x16xf32, #blocked>\n",
      "      %239 = \"tt.reduce\"(%238) <{axis = 1 : i32}> ({\n",
      "      ^bb0(%arg31: f32, %arg32: f32):\n",
      "        %373 = arith.addf %arg31, %arg32 : f32\n",
      "        tt.reduce.return %373 : f32\n",
      "      }) : (tensor<128x16xf32, #blocked>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %240 = arith.subf %176, %234 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %241 = math.exp %240 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %242 = tt.expand_dims %241 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked>\n",
      "      %243 = ttg.convert_layout %242 : tensor<128x1xf32, #blocked> -> tensor<128x1xf32, #blocked1>\n",
      "      %244 = tt.broadcast %243 : tensor<128x1xf32, #blocked1> -> tensor<128x64xf32, #blocked1>\n",
      "      %245 = arith.mulf %196, %244 : tensor<128x64xf32, #blocked1>\n",
      "      %246 = scf.if %223 -> (tensor<16x64xf16, #blocked4>) {\n",
      "        %373 = tt.splat %163 : i32 -> tensor<16x1xi32, #blocked4>\n",
      "        %374 = arith.addi %373, %81 : tensor<16x1xi32, #blocked4>\n",
      "        %375 = tt.splat %11 : i32 -> tensor<16x1xi32, #blocked4>\n",
      "        %376 = arith.cmpi slt, %374, %375 : tensor<16x1xi32, #blocked4>\n",
      "        %377 = tt.broadcast %50 : tensor<1x64xi1, #blocked4> -> tensor<16x64xi1, #blocked4>\n",
      "        %378 = tt.broadcast %376 : tensor<16x1xi1, #blocked4> -> tensor<16x64xi1, #blocked4>\n",
      "        %379 = arith.andi %377, %378 : tensor<16x64xi1, #blocked4>\n",
      "        %380 = tt.splat %arg4 : !tt.ptr<f16> -> tensor<16x64x!tt.ptr<f16>, #blocked4>\n",
      "        %381 = tt.addptr %380, %221 : tensor<16x64x!tt.ptr<f16>, #blocked4>, tensor<16x64xi32, #blocked4>\n",
      "        %382 = tt.load %381, %379, %cst_6 : tensor<16x64x!tt.ptr<f16>, #blocked4>\n",
      "        scf.yield %382 : tensor<16x64xf16, #blocked4>\n",
      "      } else {\n",
      "        %373 = tt.splat %arg4 : !tt.ptr<f16> -> tensor<16x64x!tt.ptr<f16>, #blocked4>\n",
      "        %374 = tt.addptr %373, %221 : tensor<16x64x!tt.ptr<f16>, #blocked4>, tensor<16x64xi32, #blocked4>\n",
      "        %375 = tt.load %374 : tensor<16x64x!tt.ptr<f16>, #blocked4>\n",
      "        scf.yield %375 : tensor<16x64xf16, #blocked4>\n",
      "      }\n",
      "      %247 = arith.truncf %238 : tensor<128x16xf32, #blocked> to tensor<128x16xf16, #blocked>\n",
      "      %248 = tt.fp_to_fp %247 : tensor<128x16xf16, #blocked> -> tensor<128x16xf32, #blocked>\n",
      "      %249 = ttg.local_alloc %248 : (tensor<128x16xf32, #blocked>) -> !ttg.memdesc<128x16xf32, #shared, #smem>\n",
      "      %250 = ttg.local_load %249 : !ttg.memdesc<128x16xf32, #shared, #smem> -> tensor<128x16xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>>\n",
      "      %251 = tt.fp_to_fp %246 : tensor<16x64xf16, #blocked4> -> tensor<16x64xf32, #blocked4>\n",
      "      %252 = ttg.local_alloc %251 : (tensor<16x64xf32, #blocked4>) -> !ttg.memdesc<16x64xf32, #shared1, #smem>\n",
      "      %253 = ttg.local_load %252 : !ttg.memdesc<16x64xf32, #shared1, #smem> -> tensor<16x64xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>>\n",
      "      %254 = tt.dot %250, %253, %245, inputPrecision = tf32 : tensor<128x16xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>> * tensor<16x64xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>> -> tensor<128x64xf32, #blocked1>\n",
      "      %255 = arith.mulf %198, %241 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %256 = arith.addf %255, %239 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %257 = arith.divsi %222, %c16_i32 : i32\n",
      "      %258 = tt.addptr %65, %257 : !tt.ptr<i32>, i32\n",
      "      %259 = tt.load %258 : !tt.ptr<i32>\n",
      "      %260 = arith.muli %259, %arg21 : i32\n",
      "      %261 = arith.addi %260, %66 : i32\n",
      "      %262 = tt.splat %261 : i32 -> tensor<64x1xi32, #blocked3>\n",
      "      %263 = arith.addi %262, %70 : tensor<64x1xi32, #blocked3>\n",
      "      %264 = tt.splat %222 : i32 -> tensor<1x16xi32, #blocked3>\n",
      "      %265 = tt.splat %222 : i32 -> tensor<1x16xi32, #blocked>\n",
      "      %266 = arith.addi %264, %73 : tensor<1x16xi32, #blocked3>\n",
      "      %267 = arith.addi %265, %74 : tensor<1x16xi32, #blocked>\n",
      "      %268 = arith.remsi %266, %cst_20 : tensor<1x16xi32, #blocked3>\n",
      "      %269 = arith.muli %268, %cst_21 : tensor<1x16xi32, #blocked3>\n",
      "      %270 = tt.broadcast %263 : tensor<64x1xi32, #blocked3> -> tensor<64x16xi32, #blocked3>\n",
      "      %271 = tt.broadcast %269 : tensor<1x16xi32, #blocked3> -> tensor<64x16xi32, #blocked3>\n",
      "      %272 = arith.addi %270, %271 : tensor<64x16xi32, #blocked3>\n",
      "      %273 = arith.addi %272, %76 : tensor<64x16xi32, #blocked3>\n",
      "      %274 = arith.muli %259, %arg24 : i32\n",
      "      %275 = arith.addi %274, %77 : i32\n",
      "      %276 = tt.splat %275 : i32 -> tensor<1x64xi32, #blocked4>\n",
      "      %277 = arith.addi %276, %79 : tensor<1x64xi32, #blocked4>\n",
      "      %278 = tt.broadcast %277 : tensor<1x64xi32, #blocked4> -> tensor<16x64xi32, #blocked4>\n",
      "      %279 = arith.addi %278, %82 : tensor<16x64xi32, #blocked4>\n",
      "      %280 = arith.addi %arg27, %c48_i32 : i32\n",
      "      %281 = arith.cmpi sgt, %280, %11 : i32\n",
      "      %282 = scf.if %281 -> (tensor<64x16xf16, #blocked3>) {\n",
      "        %373 = tt.expand_dims %48 {axis = 1 : i32} : tensor<64xi1, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<64x1xi1, #blocked3>\n",
      "        %374 = tt.splat %11 : i32 -> tensor<1x16xi32, #blocked3>\n",
      "        %375 = arith.cmpi slt, %266, %374 : tensor<1x16xi32, #blocked3>\n",
      "        %376 = tt.broadcast %373 : tensor<64x1xi1, #blocked3> -> tensor<64x16xi1, #blocked3>\n",
      "        %377 = tt.broadcast %375 : tensor<1x16xi1, #blocked3> -> tensor<64x16xi1, #blocked3>\n",
      "        %378 = arith.andi %376, %377 : tensor<64x16xi1, #blocked3>\n",
      "        %379 = tt.splat %arg3 : !tt.ptr<f16> -> tensor<64x16x!tt.ptr<f16>, #blocked3>\n",
      "        %380 = tt.addptr %379, %273 : tensor<64x16x!tt.ptr<f16>, #blocked3>, tensor<64x16xi32, #blocked3>\n",
      "        %381 = tt.load %380, %378, %cst_8 : tensor<64x16x!tt.ptr<f16>, #blocked3>\n",
      "        scf.yield %381 : tensor<64x16xf16, #blocked3>\n",
      "      } else {\n",
      "        %373 = tt.splat %arg3 : !tt.ptr<f16> -> tensor<64x16x!tt.ptr<f16>, #blocked3>\n",
      "        %374 = tt.addptr %373, %273 : tensor<64x16x!tt.ptr<f16>, #blocked3>, tensor<64x16xi32, #blocked3>\n",
      "        %375 = tt.load %374 : tensor<64x16x!tt.ptr<f16>, #blocked3>\n",
      "        scf.yield %375 : tensor<64x16xf16, #blocked3>\n",
      "      }\n",
      "      %283 = tt.fp_to_fp %282 : tensor<64x16xf16, #blocked3> -> tensor<64x16xf32, #blocked3>\n",
      "      %284 = ttg.local_alloc %283 : (tensor<64x16xf32, #blocked3>) -> !ttg.memdesc<64x16xf32, #shared1, #smem>\n",
      "      %285 = ttg.local_load %284 : !ttg.memdesc<64x16xf32, #shared1, #smem> -> tensor<64x16xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>\n",
      "      %286 = tt.dot %168, %285, %cst_7, inputPrecision = tf32 : tensor<128x64xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<64x16xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<128x16xf32, #blocked>\n",
      "      %287 = arith.cmpi slt, %267, %85 : tensor<1x16xi32, #blocked>\n",
      "      %288 = tt.broadcast %287 : tensor<1x16xi1, #blocked> -> tensor<128x16xi1, #blocked>\n",
      "      %289 = arith.select %288, %286, %cst_1 : tensor<128x16xi1, #blocked>, tensor<128x16xf32, #blocked>\n",
      "      %290 = arith.mulf %289, %86 : tensor<128x16xf32, #blocked>\n",
      "      %291 = \"tt.reduce\"(%290) <{axis = 1 : i32}> ({\n",
      "      ^bb0(%arg31: f32, %arg32: f32):\n",
      "        %373 = arith.maxnumf %arg31, %arg32 : f32\n",
      "        tt.reduce.return %373 : f32\n",
      "      }) : (tensor<128x16xf32, #blocked>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %292 = arith.maxnumf %234, %291 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %293 = tt.expand_dims %292 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked>\n",
      "      %294 = tt.broadcast %293 : tensor<128x1xf32, #blocked> -> tensor<128x16xf32, #blocked>\n",
      "      %295 = arith.subf %290, %294 : tensor<128x16xf32, #blocked>\n",
      "      %296 = math.exp %295 : tensor<128x16xf32, #blocked>\n",
      "      %297 = \"tt.reduce\"(%296) <{axis = 1 : i32}> ({\n",
      "      ^bb0(%arg31: f32, %arg32: f32):\n",
      "        %373 = arith.addf %arg31, %arg32 : f32\n",
      "        tt.reduce.return %373 : f32\n",
      "      }) : (tensor<128x16xf32, #blocked>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %298 = arith.subf %234, %292 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %299 = math.exp %298 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %300 = tt.expand_dims %299 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked>\n",
      "      %301 = ttg.convert_layout %300 : tensor<128x1xf32, #blocked> -> tensor<128x1xf32, #blocked1>\n",
      "      %302 = tt.broadcast %301 : tensor<128x1xf32, #blocked1> -> tensor<128x64xf32, #blocked1>\n",
      "      %303 = arith.mulf %254, %302 : tensor<128x64xf32, #blocked1>\n",
      "      %304 = scf.if %281 -> (tensor<16x64xf16, #blocked4>) {\n",
      "        %373 = tt.splat %222 : i32 -> tensor<16x1xi32, #blocked4>\n",
      "        %374 = arith.addi %373, %81 : tensor<16x1xi32, #blocked4>\n",
      "        %375 = tt.splat %11 : i32 -> tensor<16x1xi32, #blocked4>\n",
      "        %376 = arith.cmpi slt, %374, %375 : tensor<16x1xi32, #blocked4>\n",
      "        %377 = tt.broadcast %50 : tensor<1x64xi1, #blocked4> -> tensor<16x64xi1, #blocked4>\n",
      "        %378 = tt.broadcast %376 : tensor<16x1xi1, #blocked4> -> tensor<16x64xi1, #blocked4>\n",
      "        %379 = arith.andi %377, %378 : tensor<16x64xi1, #blocked4>\n",
      "        %380 = tt.splat %arg4 : !tt.ptr<f16> -> tensor<16x64x!tt.ptr<f16>, #blocked4>\n",
      "        %381 = tt.addptr %380, %279 : tensor<16x64x!tt.ptr<f16>, #blocked4>, tensor<16x64xi32, #blocked4>\n",
      "        %382 = tt.load %381, %379, %cst_6 : tensor<16x64x!tt.ptr<f16>, #blocked4>\n",
      "        scf.yield %382 : tensor<16x64xf16, #blocked4>\n",
      "      } else {\n",
      "        %373 = tt.splat %arg4 : !tt.ptr<f16> -> tensor<16x64x!tt.ptr<f16>, #blocked4>\n",
      "        %374 = tt.addptr %373, %279 : tensor<16x64x!tt.ptr<f16>, #blocked4>, tensor<16x64xi32, #blocked4>\n",
      "        %375 = tt.load %374 : tensor<16x64x!tt.ptr<f16>, #blocked4>\n",
      "        scf.yield %375 : tensor<16x64xf16, #blocked4>\n",
      "      }\n",
      "      %305 = arith.truncf %296 : tensor<128x16xf32, #blocked> to tensor<128x16xf16, #blocked>\n",
      "      %306 = tt.fp_to_fp %305 : tensor<128x16xf16, #blocked> -> tensor<128x16xf32, #blocked>\n",
      "      %307 = ttg.local_alloc %306 : (tensor<128x16xf32, #blocked>) -> !ttg.memdesc<128x16xf32, #shared, #smem>\n",
      "      %308 = ttg.local_load %307 : !ttg.memdesc<128x16xf32, #shared, #smem> -> tensor<128x16xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>>\n",
      "      %309 = tt.fp_to_fp %304 : tensor<16x64xf16, #blocked4> -> tensor<16x64xf32, #blocked4>\n",
      "      %310 = ttg.local_alloc %309 : (tensor<16x64xf32, #blocked4>) -> !ttg.memdesc<16x64xf32, #shared1, #smem>\n",
      "      %311 = ttg.local_load %310 : !ttg.memdesc<16x64xf32, #shared1, #smem> -> tensor<16x64xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>>\n",
      "      %312 = tt.dot %308, %311, %303, inputPrecision = tf32 : tensor<128x16xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>> * tensor<16x64xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>> -> tensor<128x64xf32, #blocked1>\n",
      "      %313 = arith.mulf %256, %299 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %314 = arith.addf %313, %297 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %315 = arith.divsi %280, %c16_i32 : i32\n",
      "      %316 = tt.addptr %65, %315 : !tt.ptr<i32>, i32\n",
      "      %317 = tt.load %316 : !tt.ptr<i32>\n",
      "      %318 = arith.muli %317, %arg21 : i32\n",
      "      %319 = arith.addi %318, %66 : i32\n",
      "      %320 = tt.splat %319 : i32 -> tensor<64x1xi32, #blocked3>\n",
      "      %321 = arith.addi %320, %70 : tensor<64x1xi32, #blocked3>\n",
      "      %322 = tt.splat %280 : i32 -> tensor<1x16xi32, #blocked3>\n",
      "      %323 = tt.splat %280 : i32 -> tensor<1x16xi32, #blocked>\n",
      "      %324 = arith.addi %322, %73 : tensor<1x16xi32, #blocked3>\n",
      "      %325 = arith.addi %323, %74 : tensor<1x16xi32, #blocked>\n",
      "      %326 = arith.remsi %324, %cst_20 : tensor<1x16xi32, #blocked3>\n",
      "      %327 = arith.muli %326, %cst_21 : tensor<1x16xi32, #blocked3>\n",
      "      %328 = tt.broadcast %321 : tensor<64x1xi32, #blocked3> -> tensor<64x16xi32, #blocked3>\n",
      "      %329 = tt.broadcast %327 : tensor<1x16xi32, #blocked3> -> tensor<64x16xi32, #blocked3>\n",
      "      %330 = arith.addi %328, %329 : tensor<64x16xi32, #blocked3>\n",
      "      %331 = arith.addi %330, %76 : tensor<64x16xi32, #blocked3>\n",
      "      %332 = arith.muli %317, %arg24 : i32\n",
      "      %333 = arith.addi %332, %77 : i32\n",
      "      %334 = tt.splat %333 : i32 -> tensor<1x64xi32, #blocked4>\n",
      "      %335 = arith.addi %334, %79 : tensor<1x64xi32, #blocked4>\n",
      "      %336 = tt.broadcast %335 : tensor<1x64xi32, #blocked4> -> tensor<16x64xi32, #blocked4>\n",
      "      %337 = arith.addi %336, %82 : tensor<16x64xi32, #blocked4>\n",
      "      %338 = arith.addi %arg27, %c64_i32 : i32\n",
      "      %339 = arith.cmpi sgt, %338, %11 : i32\n",
      "      %340 = scf.if %339 -> (tensor<64x16xf16, #blocked3>) {\n",
      "        %373 = tt.expand_dims %48 {axis = 1 : i32} : tensor<64xi1, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<64x1xi1, #blocked3>\n",
      "        %374 = tt.splat %11 : i32 -> tensor<1x16xi32, #blocked3>\n",
      "        %375 = arith.cmpi slt, %324, %374 : tensor<1x16xi32, #blocked3>\n",
      "        %376 = tt.broadcast %373 : tensor<64x1xi1, #blocked3> -> tensor<64x16xi1, #blocked3>\n",
      "        %377 = tt.broadcast %375 : tensor<1x16xi1, #blocked3> -> tensor<64x16xi1, #blocked3>\n",
      "        %378 = arith.andi %376, %377 : tensor<64x16xi1, #blocked3>\n",
      "        %379 = tt.splat %arg3 : !tt.ptr<f16> -> tensor<64x16x!tt.ptr<f16>, #blocked3>\n",
      "        %380 = tt.addptr %379, %331 : tensor<64x16x!tt.ptr<f16>, #blocked3>, tensor<64x16xi32, #blocked3>\n",
      "        %381 = tt.load %380, %378, %cst_8 : tensor<64x16x!tt.ptr<f16>, #blocked3>\n",
      "        scf.yield %381 : tensor<64x16xf16, #blocked3>\n",
      "      } else {\n",
      "        %373 = tt.splat %arg3 : !tt.ptr<f16> -> tensor<64x16x!tt.ptr<f16>, #blocked3>\n",
      "        %374 = tt.addptr %373, %331 : tensor<64x16x!tt.ptr<f16>, #blocked3>, tensor<64x16xi32, #blocked3>\n",
      "        %375 = tt.load %374 : tensor<64x16x!tt.ptr<f16>, #blocked3>\n",
      "        scf.yield %375 : tensor<64x16xf16, #blocked3>\n",
      "      }\n",
      "      %341 = tt.fp_to_fp %340 : tensor<64x16xf16, #blocked3> -> tensor<64x16xf32, #blocked3>\n",
      "      %342 = ttg.local_alloc %341 : (tensor<64x16xf32, #blocked3>) -> !ttg.memdesc<64x16xf32, #shared1, #smem>\n",
      "      %343 = ttg.local_load %342 : !ttg.memdesc<64x16xf32, #shared1, #smem> -> tensor<64x16xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>\n",
      "      %344 = tt.dot %168, %343, %cst_7, inputPrecision = tf32 : tensor<128x64xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<64x16xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<128x16xf32, #blocked>\n",
      "      %345 = arith.cmpi slt, %325, %85 : tensor<1x16xi32, #blocked>\n",
      "      %346 = tt.broadcast %345 : tensor<1x16xi1, #blocked> -> tensor<128x16xi1, #blocked>\n",
      "      %347 = arith.select %346, %344, %cst_1 : tensor<128x16xi1, #blocked>, tensor<128x16xf32, #blocked>\n",
      "      %348 = arith.mulf %347, %86 : tensor<128x16xf32, #blocked>\n",
      "      %349 = \"tt.reduce\"(%348) <{axis = 1 : i32}> ({\n",
      "      ^bb0(%arg31: f32, %arg32: f32):\n",
      "        %373 = arith.maxnumf %arg31, %arg32 : f32\n",
      "        tt.reduce.return %373 : f32\n",
      "      }) : (tensor<128x16xf32, #blocked>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %350 = arith.maxnumf %292, %349 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %351 = tt.expand_dims %350 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked>\n",
      "      %352 = tt.broadcast %351 : tensor<128x1xf32, #blocked> -> tensor<128x16xf32, #blocked>\n",
      "      %353 = arith.subf %348, %352 : tensor<128x16xf32, #blocked>\n",
      "      %354 = math.exp %353 : tensor<128x16xf32, #blocked>\n",
      "      %355 = \"tt.reduce\"(%354) <{axis = 1 : i32}> ({\n",
      "      ^bb0(%arg31: f32, %arg32: f32):\n",
      "        %373 = arith.addf %arg31, %arg32 : f32\n",
      "        tt.reduce.return %373 : f32\n",
      "      }) : (tensor<128x16xf32, #blocked>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %356 = arith.subf %292, %350 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %357 = math.exp %356 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %358 = tt.expand_dims %357 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked>\n",
      "      %359 = ttg.convert_layout %358 : tensor<128x1xf32, #blocked> -> tensor<128x1xf32, #blocked1>\n",
      "      %360 = tt.broadcast %359 : tensor<128x1xf32, #blocked1> -> tensor<128x64xf32, #blocked1>\n",
      "      %361 = arith.mulf %312, %360 : tensor<128x64xf32, #blocked1>\n",
      "      %362 = scf.if %339 -> (tensor<16x64xf16, #blocked4>) {\n",
      "        %373 = tt.splat %280 : i32 -> tensor<16x1xi32, #blocked4>\n",
      "        %374 = arith.addi %373, %81 : tensor<16x1xi32, #blocked4>\n",
      "        %375 = tt.splat %11 : i32 -> tensor<16x1xi32, #blocked4>\n",
      "        %376 = arith.cmpi slt, %374, %375 : tensor<16x1xi32, #blocked4>\n",
      "        %377 = tt.broadcast %50 : tensor<1x64xi1, #blocked4> -> tensor<16x64xi1, #blocked4>\n",
      "        %378 = tt.broadcast %376 : tensor<16x1xi1, #blocked4> -> tensor<16x64xi1, #blocked4>\n",
      "        %379 = arith.andi %377, %378 : tensor<16x64xi1, #blocked4>\n",
      "        %380 = tt.splat %arg4 : !tt.ptr<f16> -> tensor<16x64x!tt.ptr<f16>, #blocked4>\n",
      "        %381 = tt.addptr %380, %337 : tensor<16x64x!tt.ptr<f16>, #blocked4>, tensor<16x64xi32, #blocked4>\n",
      "        %382 = tt.load %381, %379, %cst_6 : tensor<16x64x!tt.ptr<f16>, #blocked4>\n",
      "        scf.yield %382 : tensor<16x64xf16, #blocked4>\n",
      "      } else {\n",
      "        %373 = tt.splat %arg4 : !tt.ptr<f16> -> tensor<16x64x!tt.ptr<f16>, #blocked4>\n",
      "        %374 = tt.addptr %373, %337 : tensor<16x64x!tt.ptr<f16>, #blocked4>, tensor<16x64xi32, #blocked4>\n",
      "        %375 = tt.load %374 : tensor<16x64x!tt.ptr<f16>, #blocked4>\n",
      "        scf.yield %375 : tensor<16x64xf16, #blocked4>\n",
      "      }\n",
      "      %363 = arith.truncf %354 : tensor<128x16xf32, #blocked> to tensor<128x16xf16, #blocked>\n",
      "      %364 = tt.fp_to_fp %363 : tensor<128x16xf16, #blocked> -> tensor<128x16xf32, #blocked>\n",
      "      %365 = ttg.local_alloc %364 : (tensor<128x16xf32, #blocked>) -> !ttg.memdesc<128x16xf32, #shared, #smem>\n",
      "      %366 = ttg.local_load %365 : !ttg.memdesc<128x16xf32, #shared, #smem> -> tensor<128x16xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>>\n",
      "      %367 = tt.fp_to_fp %362 : tensor<16x64xf16, #blocked4> -> tensor<16x64xf32, #blocked4>\n",
      "      %368 = ttg.local_alloc %367 : (tensor<16x64xf32, #blocked4>) -> !ttg.memdesc<16x64xf32, #shared1, #smem>\n",
      "      %369 = ttg.local_load %368 : !ttg.memdesc<16x64xf32, #shared1, #smem> -> tensor<16x64xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>>\n",
      "      %370 = tt.dot %366, %369, %361, inputPrecision = tf32 : tensor<128x16xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>> * tensor<16x64xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>> -> tensor<128x64xf32, #blocked1>\n",
      "      %371 = arith.mulf %314, %357 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %372 = arith.addf %371, %355 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      scf.yield %370, %372, %350 : tensor<128x64xf32, #blocked1>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "    } {tt.divisibility_arg1 = dense<16> : tensor<1xi32>}\n",
      "    %88:3 = scf.for %arg27 = %63 to %11 step %c16_i32 iter_args(%arg28 = %87#0, %arg29 = %87#1, %arg30 = %87#2) -> (tensor<128x64xf32, #blocked1>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>)  : i32 {\n",
      "      %140 = arith.divsi %arg27, %c16_i32 : i32\n",
      "      %141 = tt.addptr %65, %140 : !tt.ptr<i32>, i32\n",
      "      %142 = tt.load %141 : !tt.ptr<i32>\n",
      "      %143 = arith.muli %142, %arg21 : i32\n",
      "      %144 = arith.addi %143, %66 : i32\n",
      "      %145 = tt.splat %144 : i32 -> tensor<64x1xi32, #blocked3>\n",
      "      %146 = arith.addi %145, %70 : tensor<64x1xi32, #blocked3>\n",
      "      %147 = tt.splat %arg27 : i32 -> tensor<1x16xi32, #blocked3>\n",
      "      %148 = tt.splat %arg27 : i32 -> tensor<1x16xi32, #blocked>\n",
      "      %149 = arith.addi %147, %73 : tensor<1x16xi32, #blocked3>\n",
      "      %150 = arith.addi %148, %74 : tensor<1x16xi32, #blocked>\n",
      "      %151 = arith.remsi %149, %cst_20 : tensor<1x16xi32, #blocked3>\n",
      "      %152 = arith.muli %151, %cst_21 : tensor<1x16xi32, #blocked3>\n",
      "      %153 = tt.broadcast %146 : tensor<64x1xi32, #blocked3> -> tensor<64x16xi32, #blocked3>\n",
      "      %154 = tt.broadcast %152 : tensor<1x16xi32, #blocked3> -> tensor<64x16xi32, #blocked3>\n",
      "      %155 = arith.addi %153, %154 : tensor<64x16xi32, #blocked3>\n",
      "      %156 = arith.addi %155, %76 : tensor<64x16xi32, #blocked3>\n",
      "      %157 = arith.muli %142, %arg24 : i32\n",
      "      %158 = arith.addi %157, %77 : i32\n",
      "      %159 = tt.splat %158 : i32 -> tensor<1x64xi32, #blocked4>\n",
      "      %160 = arith.addi %159, %79 : tensor<1x64xi32, #blocked4>\n",
      "      %161 = tt.broadcast %160 : tensor<1x64xi32, #blocked4> -> tensor<16x64xi32, #blocked4>\n",
      "      %162 = arith.addi %161, %82 : tensor<16x64xi32, #blocked4>\n",
      "      %163 = arith.addi %arg27, %c16_i32 : i32\n",
      "      %164 = arith.cmpi sgt, %163, %11 : i32\n",
      "      %165 = scf.if %164 -> (tensor<64x16xf16, #blocked3>) {\n",
      "        %199 = tt.expand_dims %48 {axis = 1 : i32} : tensor<64xi1, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<64x1xi1, #blocked3>\n",
      "        %200 = tt.splat %11 : i32 -> tensor<1x16xi32, #blocked3>\n",
      "        %201 = arith.cmpi slt, %149, %200 : tensor<1x16xi32, #blocked3>\n",
      "        %202 = tt.broadcast %199 : tensor<64x1xi1, #blocked3> -> tensor<64x16xi1, #blocked3>\n",
      "        %203 = tt.broadcast %201 : tensor<1x16xi1, #blocked3> -> tensor<64x16xi1, #blocked3>\n",
      "        %204 = arith.andi %202, %203 : tensor<64x16xi1, #blocked3>\n",
      "        %205 = tt.splat %arg3 : !tt.ptr<f16> -> tensor<64x16x!tt.ptr<f16>, #blocked3>\n",
      "        %206 = tt.addptr %205, %156 : tensor<64x16x!tt.ptr<f16>, #blocked3>, tensor<64x16xi32, #blocked3>\n",
      "        %207 = tt.load %206, %204, %cst_8 : tensor<64x16x!tt.ptr<f16>, #blocked3>\n",
      "        scf.yield %207 : tensor<64x16xf16, #blocked3>\n",
      "      } else {\n",
      "        %199 = tt.splat %arg3 : !tt.ptr<f16> -> tensor<64x16x!tt.ptr<f16>, #blocked3>\n",
      "        %200 = tt.addptr %199, %156 : tensor<64x16x!tt.ptr<f16>, #blocked3>, tensor<64x16xi32, #blocked3>\n",
      "        %201 = tt.load %200 : tensor<64x16x!tt.ptr<f16>, #blocked3>\n",
      "        scf.yield %201 : tensor<64x16xf16, #blocked3>\n",
      "      }\n",
      "      %166 = tt.fp_to_fp %165 : tensor<64x16xf16, #blocked3> -> tensor<64x16xf32, #blocked3>\n",
      "      %167 = ttg.local_alloc %166 : (tensor<64x16xf32, #blocked3>) -> !ttg.memdesc<64x16xf32, #shared1, #smem>\n",
      "      %168 = ttg.local_load %84 : !ttg.memdesc<128x64xf32, #shared, #smem> -> tensor<128x64xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked}>>\n",
      "      %169 = ttg.local_load %167 : !ttg.memdesc<64x16xf32, #shared1, #smem> -> tensor<64x16xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>\n",
      "      %170 = tt.dot %168, %169, %cst_7, inputPrecision = tf32 : tensor<128x64xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<64x16xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<128x16xf32, #blocked>\n",
      "      %171 = arith.cmpi slt, %150, %85 : tensor<1x16xi32, #blocked>\n",
      "      %172 = tt.broadcast %171 : tensor<1x16xi1, #blocked> -> tensor<128x16xi1, #blocked>\n",
      "      %173 = arith.select %172, %170, %cst_1 : tensor<128x16xi1, #blocked>, tensor<128x16xf32, #blocked>\n",
      "      %174 = arith.mulf %173, %86 : tensor<128x16xf32, #blocked>\n",
      "      %175 = \"tt.reduce\"(%174) <{axis = 1 : i32}> ({\n",
      "      ^bb0(%arg31: f32, %arg32: f32):\n",
      "        %199 = arith.maxnumf %arg31, %arg32 : f32\n",
      "        tt.reduce.return %199 : f32\n",
      "      }) : (tensor<128x16xf32, #blocked>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %176 = arith.maxnumf %arg30, %175 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %177 = tt.expand_dims %176 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked>\n",
      "      %178 = tt.broadcast %177 : tensor<128x1xf32, #blocked> -> tensor<128x16xf32, #blocked>\n",
      "      %179 = arith.subf %174, %178 : tensor<128x16xf32, #blocked>\n",
      "      %180 = math.exp %179 : tensor<128x16xf32, #blocked>\n",
      "      %181 = \"tt.reduce\"(%180) <{axis = 1 : i32}> ({\n",
      "      ^bb0(%arg31: f32, %arg32: f32):\n",
      "        %199 = arith.addf %arg31, %arg32 : f32\n",
      "        tt.reduce.return %199 : f32\n",
      "      }) : (tensor<128x16xf32, #blocked>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %182 = arith.subf %arg30, %176 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %183 = math.exp %182 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %184 = tt.expand_dims %183 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked>\n",
      "      %185 = ttg.convert_layout %184 : tensor<128x1xf32, #blocked> -> tensor<128x1xf32, #blocked1>\n",
      "      %186 = tt.broadcast %185 : tensor<128x1xf32, #blocked1> -> tensor<128x64xf32, #blocked1>\n",
      "      %187 = arith.mulf %arg28, %186 : tensor<128x64xf32, #blocked1>\n",
      "      %188 = scf.if %164 -> (tensor<16x64xf16, #blocked4>) {\n",
      "        %199 = tt.splat %arg27 : i32 -> tensor<16x1xi32, #blocked4>\n",
      "        %200 = arith.addi %199, %81 : tensor<16x1xi32, #blocked4>\n",
      "        %201 = tt.splat %11 : i32 -> tensor<16x1xi32, #blocked4>\n",
      "        %202 = arith.cmpi slt, %200, %201 : tensor<16x1xi32, #blocked4>\n",
      "        %203 = tt.broadcast %50 : tensor<1x64xi1, #blocked4> -> tensor<16x64xi1, #blocked4>\n",
      "        %204 = tt.broadcast %202 : tensor<16x1xi1, #blocked4> -> tensor<16x64xi1, #blocked4>\n",
      "        %205 = arith.andi %203, %204 : tensor<16x64xi1, #blocked4>\n",
      "        %206 = tt.splat %arg4 : !tt.ptr<f16> -> tensor<16x64x!tt.ptr<f16>, #blocked4>\n",
      "        %207 = tt.addptr %206, %162 : tensor<16x64x!tt.ptr<f16>, #blocked4>, tensor<16x64xi32, #blocked4>\n",
      "        %208 = tt.load %207, %205, %cst_6 : tensor<16x64x!tt.ptr<f16>, #blocked4>\n",
      "        scf.yield %208 : tensor<16x64xf16, #blocked4>\n",
      "      } else {\n",
      "        %199 = tt.splat %arg4 : !tt.ptr<f16> -> tensor<16x64x!tt.ptr<f16>, #blocked4>\n",
      "        %200 = tt.addptr %199, %162 : tensor<16x64x!tt.ptr<f16>, #blocked4>, tensor<16x64xi32, #blocked4>\n",
      "        %201 = tt.load %200 : tensor<16x64x!tt.ptr<f16>, #blocked4>\n",
      "        scf.yield %201 : tensor<16x64xf16, #blocked4>\n",
      "      }\n",
      "      %189 = arith.truncf %180 : tensor<128x16xf32, #blocked> to tensor<128x16xf16, #blocked>\n",
      "      %190 = tt.fp_to_fp %189 : tensor<128x16xf16, #blocked> -> tensor<128x16xf32, #blocked>\n",
      "      %191 = ttg.local_alloc %190 : (tensor<128x16xf32, #blocked>) -> !ttg.memdesc<128x16xf32, #shared, #smem>\n",
      "      %192 = ttg.local_load %191 : !ttg.memdesc<128x16xf32, #shared, #smem> -> tensor<128x16xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>>\n",
      "      %193 = tt.fp_to_fp %188 : tensor<16x64xf16, #blocked4> -> tensor<16x64xf32, #blocked4>\n",
      "      %194 = ttg.local_alloc %193 : (tensor<16x64xf32, #blocked4>) -> !ttg.memdesc<16x64xf32, #shared1, #smem>\n",
      "      %195 = ttg.local_load %194 : !ttg.memdesc<16x64xf32, #shared1, #smem> -> tensor<16x64xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>>\n",
      "      %196 = tt.dot %192, %195, %187, inputPrecision = tf32 : tensor<128x16xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>> * tensor<16x64xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>> -> tensor<128x64xf32, #blocked1>\n",
      "      %197 = arith.mulf %arg29, %183 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      %198 = arith.addf %197, %181 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "      scf.yield %196, %198, %176 : tensor<128x64xf32, #blocked1>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "    } {tt.divisibility_arg1 = dense<16> : tensor<1xi32>, tt.num_stages = 1 : i32}\n",
      "    %89 = ttg.convert_layout %88#2 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>>\n",
      "    %90 = ttg.convert_layout %88#1 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>>\n",
      "    %91 = tt.splat %arg15 : i32 -> tensor<1x64xi32, #blocked3>\n",
      "    %92 = arith.muli %35, %91 : tensor<1x64xi32, #blocked3>\n",
      "    %93 = arith.muli %3, %arg16 : i32\n",
      "    %94 = tt.splat %93 : i32 -> tensor<1x64xi32, #blocked3>\n",
      "    %95 = arith.addi %92, %94 : tensor<1x64xi32, #blocked3>\n",
      "    %96 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>\n",
      "    %97 = tt.expand_dims %96 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<64x1xi32, #blocked2>\n",
      "    %98 = tt.broadcast %95 : tensor<1x64xi32, #blocked3> -> tensor<64x64xi32, #blocked3>\n",
      "    %99 = tt.broadcast %67 : tensor<64x1xi32, #blocked3> -> tensor<64x64xi32, #blocked3>\n",
      "    %100 = arith.addi %98, %99 : tensor<64x64xi32, #blocked3>\n",
      "    %101 = tt.splat %arg17 : i32 -> tensor<64x1xi32, #blocked2>\n",
      "    %102 = arith.muli %97, %101 : tensor<64x1xi32, #blocked2>\n",
      "    %103 = arith.muli %3, %arg18 : i32\n",
      "    %104 = tt.splat %103 : i32 -> tensor<64x1xi32, #blocked2>\n",
      "    %105 = arith.addi %102, %104 : tensor<64x1xi32, #blocked2>\n",
      "    %106 = tt.broadcast %105 : tensor<64x1xi32, #blocked2> -> tensor<64x64xi32, #blocked2>\n",
      "    %107 = tt.broadcast %33 : tensor<1x64xi32, #blocked2> -> tensor<64x64xi32, #blocked2>\n",
      "    %108 = arith.addi %106, %107 : tensor<64x64xi32, #blocked2>\n",
      "    %109 = tt.splat %arg1 : !tt.ptr<f16> -> tensor<64x64x!tt.ptr<f16>, #blocked3>\n",
      "    %110 = tt.addptr %109, %100 : tensor<64x64x!tt.ptr<f16>, #blocked3>, tensor<64x64xi32, #blocked3>\n",
      "    %111 = tt.splat %arg2 : !tt.ptr<f16> -> tensor<64x64x!tt.ptr<f16>, #blocked2>\n",
      "    %112 = tt.addptr %111, %108 : tensor<64x64x!tt.ptr<f16>, #blocked2>, tensor<64x64xi32, #blocked2>\n",
      "    %113 = arith.cmpi slt, %12, %10 : i32\n",
      "    %114 = arith.extui %113 : i1 to i32\n",
      "    %115 = arith.addi %2, %c1_i32 : i32\n",
      "    %116 = arith.muli %114, %115 : i32\n",
      "    %117 = arith.muli %116, %c128_i32 : i32\n",
      "    %118 = tt.expand_dims %48 {axis = 1 : i32} : tensor<64xi1, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<64x1xi1, #blocked3>\n",
      "    %119 = tt.splat %10 : i32 -> tensor<1x64xi32, #blocked3>\n",
      "    %120 = tt.broadcast %118 : tensor<64x1xi1, #blocked3> -> tensor<64x64xi1, #blocked3>\n",
      "    %121 = tt.splat %arg6 : f32 -> tensor<128x64xf32, #blocked1>\n",
      "    %122 = tt.broadcast %23 : tensor<128x1xi32, #blocked1> -> tensor<128x64xi32, #blocked1>\n",
      "    %123 = tt.splat %10 : i32 -> tensor<64x1xi32, #blocked2>\n",
      "    %124 = tt.broadcast %49 : tensor<1x64xi1, #blocked2> -> tensor<64x64xi1, #blocked2>\n",
      "    %125:3 = scf.for %arg27 = %c0_i32 to %117 step %c64_i32 iter_args(%arg28 = %88#0, %arg29 = %90, %arg30 = %89) -> (tensor<128x64xf32, #blocked1>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>>)  : i32 {\n",
      "      %140 = tt.splat %arg27 : i32 -> tensor<1x64xi32, #blocked3>\n",
      "      %141 = tt.splat %arg27 : i32 -> tensor<1x64xi32, #blocked1>\n",
      "      %142 = arith.addi %140, %35 : tensor<1x64xi32, #blocked3>\n",
      "      %143 = arith.addi %141, %36 : tensor<1x64xi32, #blocked1>\n",
      "      %144 = arith.cmpi slt, %142, %119 : tensor<1x64xi32, #blocked3>\n",
      "      %145 = tt.broadcast %144 : tensor<1x64xi1, #blocked3> -> tensor<64x64xi1, #blocked3>\n",
      "      %146 = arith.andi %120, %145 : tensor<64x64xi1, #blocked3>\n",
      "      %147 = arith.addi %7, %arg27 : i32\n",
      "      %148 = arith.muli %147, %arg15 : i32\n",
      "      %149 = tt.splat %148 : i32 -> tensor<64x64xi32, #blocked3>\n",
      "      %150 = tt.addptr %110, %149 : tensor<64x64x!tt.ptr<f16>, #blocked3>, tensor<64x64xi32, #blocked3>\n",
      "      %151 = tt.load %150, %146, %cst_5 : tensor<64x64x!tt.ptr<f16>, #blocked3>\n",
      "      %152 = tt.fp_to_fp %151 : tensor<64x64xf16, #blocked3> -> tensor<64x64xf32, #blocked3>\n",
      "      %153 = ttg.local_alloc %152 : (tensor<64x64xf32, #blocked3>) -> !ttg.memdesc<64x64xf32, #shared1, #smem>\n",
      "      %154 = ttg.local_load %84 : !ttg.memdesc<128x64xf32, #shared, #smem> -> tensor<128x64xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>>\n",
      "      %155 = ttg.local_load %153 : !ttg.memdesc<64x64xf32, #shared1, #smem> -> tensor<64x64xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>>\n",
      "      %156 = tt.dot %154, %155, %cst_3, inputPrecision = tf32 : tensor<128x64xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>> * tensor<64x64xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>> -> tensor<128x64xf32, #blocked1>\n",
      "      %157 = arith.mulf %156, %121 : tensor<128x64xf32, #blocked1>\n",
      "      %158 = tt.broadcast %143 : tensor<1x64xi32, #blocked1> -> tensor<128x64xi32, #blocked1>\n",
      "      %159 = arith.cmpi sge, %122, %158 : tensor<128x64xi32, #blocked1>\n",
      "      %160 = arith.select %159, %157, %cst_2 : tensor<128x64xi1, #blocked1>, tensor<128x64xf32, #blocked1>\n",
      "      %161 = \"tt.reduce\"(%160) <{axis = 1 : i32}> ({\n",
      "      ^bb0(%arg31: f32, %arg32: f32):\n",
      "        %192 = arith.maxnumf %arg31, %arg32 : f32\n",
      "        tt.reduce.return %192 : f32\n",
      "      }) : (tensor<128x64xf32, #blocked1>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>>\n",
      "      %162 = arith.maxnumf %arg30, %161 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>>\n",
      "      %163 = tt.expand_dims %162 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<128x1xf32, #blocked1>\n",
      "      %164 = tt.broadcast %163 : tensor<128x1xf32, #blocked1> -> tensor<128x64xf32, #blocked1>\n",
      "      %165 = arith.subf %160, %164 : tensor<128x64xf32, #blocked1>\n",
      "      %166 = math.exp %165 : tensor<128x64xf32, #blocked1>\n",
      "      %167 = \"tt.reduce\"(%166) <{axis = 1 : i32}> ({\n",
      "      ^bb0(%arg31: f32, %arg32: f32):\n",
      "        %192 = arith.addf %arg31, %arg32 : f32\n",
      "        tt.reduce.return %192 : f32\n",
      "      }) : (tensor<128x64xf32, #blocked1>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>>\n",
      "      %168 = arith.subf %arg30, %162 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>>\n",
      "      %169 = math.exp %168 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>>\n",
      "      %170 = tt.expand_dims %169 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<128x1xf32, #blocked1>\n",
      "      %171 = tt.broadcast %170 : tensor<128x1xf32, #blocked1> -> tensor<128x64xf32, #blocked1>\n",
      "      %172 = arith.mulf %arg28, %171 : tensor<128x64xf32, #blocked1>\n",
      "      %173 = tt.splat %arg27 : i32 -> tensor<64x1xi32, #blocked2>\n",
      "      %174 = arith.addi %173, %97 : tensor<64x1xi32, #blocked2>\n",
      "      %175 = arith.cmpi slt, %174, %123 : tensor<64x1xi32, #blocked2>\n",
      "      %176 = tt.broadcast %175 : tensor<64x1xi1, #blocked2> -> tensor<64x64xi1, #blocked2>\n",
      "      %177 = arith.andi %124, %176 : tensor<64x64xi1, #blocked2>\n",
      "      %178 = arith.muli %147, %arg17 : i32\n",
      "      %179 = tt.splat %178 : i32 -> tensor<64x64xi32, #blocked2>\n",
      "      %180 = tt.addptr %112, %179 : tensor<64x64x!tt.ptr<f16>, #blocked2>, tensor<64x64xi32, #blocked2>\n",
      "      %181 = tt.load %180, %177, %cst_4 : tensor<64x64x!tt.ptr<f16>, #blocked2>\n",
      "      %182 = arith.truncf %166 : tensor<128x64xf32, #blocked1> to tensor<128x64xf16, #blocked1>\n",
      "      %183 = tt.fp_to_fp %182 : tensor<128x64xf16, #blocked1> -> tensor<128x64xf32, #blocked1>\n",
      "      %184 = ttg.local_alloc %183 : (tensor<128x64xf32, #blocked1>) -> !ttg.memdesc<128x64xf32, #shared, #smem>\n",
      "      %185 = ttg.local_load %184 : !ttg.memdesc<128x64xf32, #shared, #smem> -> tensor<128x64xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>>\n",
      "      %186 = tt.fp_to_fp %181 : tensor<64x64xf16, #blocked2> -> tensor<64x64xf32, #blocked2>\n",
      "      %187 = ttg.local_alloc %186 : (tensor<64x64xf32, #blocked2>) -> !ttg.memdesc<64x64xf32, #shared, #smem>\n",
      "      %188 = ttg.local_load %187 : !ttg.memdesc<64x64xf32, #shared, #smem> -> tensor<64x64xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>>\n",
      "      %189 = tt.dot %185, %188, %172, inputPrecision = tf32 : tensor<128x64xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>> * tensor<64x64xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>> -> tensor<128x64xf32, #blocked1>\n",
      "      %190 = arith.mulf %arg29, %169 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>>\n",
      "      %191 = arith.addf %190, %167 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>>\n",
      "      scf.yield %189, %191, %162 : tensor<128x64xf32, #blocked1>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>>\n",
      "    } {tt.divisibility_arg1 = dense<64> : tensor<1xi32>, tt.loop_unroll_factor = 1 : i32}\n",
      "    %126 = tt.expand_dims %125#1 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<128x1xf32, #blocked1>\n",
      "    %127 = tt.broadcast %126 : tensor<128x1xf32, #blocked1> -> tensor<128x64xf32, #blocked1>\n",
      "    %128 = arith.divf %125#0, %127 : tensor<128x64xf32, #blocked1>\n",
      "    %129 = tt.splat %arg19 : i32 -> tensor<128x1xi32, #blocked2>\n",
      "    %130 = arith.muli %25, %129 : tensor<128x1xi32, #blocked2>\n",
      "    %131 = arith.muli %1, %arg20 : i32\n",
      "    %132 = tt.splat %131 : i32 -> tensor<128x1xi32, #blocked2>\n",
      "    %133 = arith.addi %130, %132 : tensor<128x1xi32, #blocked2>\n",
      "    %134 = tt.broadcast %133 : tensor<128x1xi32, #blocked2> -> tensor<128x64xi32, #blocked2>\n",
      "    %135 = arith.addi %134, %38 : tensor<128x64xi32, #blocked2>\n",
      "    %136 = tt.splat %arg11 : !tt.ptr<f16> -> tensor<128x64x!tt.ptr<f16>, #blocked2>\n",
      "    %137 = tt.addptr %136, %135 : tensor<128x64x!tt.ptr<f16>, #blocked2>, tensor<128x64xi32, #blocked2>\n",
      "    %138 = arith.truncf %128 : tensor<128x64xf32, #blocked1> to tensor<128x64xf16, #blocked1>\n",
      "    %139 = ttg.convert_layout %138 : tensor<128x64xf16, #blocked1> -> tensor<128x64xf16, #blocked2>\n",
      "    tt.store %137, %139, %55 : tensor<128x64x!tt.ptr<f16>, #blocked2>\n",
      "    tt.return\n",
      "  }\n",
      "}\n",
      "\n",
      "{-#\n",
      "  external_resources: {\n",
      "    mlir_reproducer: {\n",
      "      pipeline: \"builtin.module(triton-nvidia-mma-lowering, tritongpu-combine-tensor-select-and-if, tritongpu-allocate-warp-groups, convert-scf-to-cf, allocate-shared-memory, triton-tensor-memory-allocation, tritongpu-global-scratch-memory-allocation, convert-triton-gpu-to-llvm{compute-capability=75 ptx-version=84}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, convert-nv-gpu-to-llvm, convert-warp-specialize-to-llvm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce, enable-line-info)\",\n",
      "      disable_threading: false,\n",
      "      verify_each: true\n",
      "    }\n",
      "  }\n",
      "#-}\n",
      "/home/simonenni/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/attention/ops/prefix_prefill.py:36:0: error: Failures have been detected while processing an MLIR pass pipeline\n",
      "/home/simonenni/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/attention/ops/prefix_prefill.py:36:0: note: Pipeline failed while executing [`ConvertTritonGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "PassManager::run failed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(case)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOutput:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43magent_evaluated\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcase\u001b[49m\u001b[43m.\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mHuggingFaceAgent.run\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m     48\u001b[39m guided_params = GuidedDecodingParams(regex=\u001b[38;5;28mself\u001b[39m.answer_pattern)\n\u001b[32m     49\u001b[39m sampling_params = SamplingParams(guided_decoding=guided_params)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m model_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampling_params\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m#model_output = self.model(prompt, answer_pattern, max_new_tokens=500)\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m#inputs = self.tokenizer(prompt, return_tensors=\"pt\")\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m#inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m#model_output = self.model.generate(**inputs, max_new_tokens=500)\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m#model_output = self.tokenizer.decode(model_output[0], skip_special_tokens=True)\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/utils/__init__.py:1514\u001b[39m, in \u001b[36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1507\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1509\u001b[39m         warnings.warn(\n\u001b[32m   1510\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1511\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1512\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1514\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py:505\u001b[39m, in \u001b[36mLLM.generate\u001b[39m\u001b[34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, guided_options_request, priority)\u001b[39m\n\u001b[32m    492\u001b[39m lora_request = \u001b[38;5;28mself\u001b[39m._get_modality_specific_lora_reqs(\n\u001b[32m    493\u001b[39m     parsed_prompts, lora_request)\n\u001b[32m    495\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_and_add_requests(\n\u001b[32m    496\u001b[39m     prompts=parsed_prompts,\n\u001b[32m    497\u001b[39m     params=sampling_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    502\u001b[39m     priority=priority,\n\u001b[32m    503\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine_class.validate_outputs(outputs, RequestOutput)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py:1701\u001b[39m, in \u001b[36mLLM._run_engine\u001b[39m\u001b[34m(self, use_tqdm)\u001b[39m\n\u001b[32m   1699\u001b[39m total_out_toks = \u001b[32m0\u001b[39m\n\u001b[32m   1700\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm_engine.has_unfinished_requests():\n\u001b[32m-> \u001b[39m\u001b[32m1701\u001b[39m     step_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1702\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[32m   1703\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m output.finished:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:1334\u001b[39m, in \u001b[36mLLMEngine.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1330\u001b[39m     execute_model_req.async_callback = \u001b[38;5;28mself\u001b[39m.async_callbacks[\n\u001b[32m   1331\u001b[39m         virtual_engine]\n\u001b[32m   1333\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1334\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1336\u001b[39m     \u001b[38;5;28mself\u001b[39m._skip_scheduling_next_step = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1337\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InputProcessingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1338\u001b[39m     \u001b[38;5;66;03m# The input for this request cannot be processed, so we must\u001b[39;00m\n\u001b[32m   1339\u001b[39m     \u001b[38;5;66;03m# abort it. If there are remaining requests in the batch that\u001b[39;00m\n\u001b[32m   1340\u001b[39m     \u001b[38;5;66;03m# have been scheduled, they will be retried on the next step.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/executor/executor_base.py:146\u001b[39m, in \u001b[36mExecutorBase.execute_model\u001b[39m\u001b[34m(self, execute_model_req)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute_model\u001b[39m(\n\u001b[32m    144\u001b[39m     \u001b[38;5;28mself\u001b[39m, execute_model_req: ExecuteModelRequest\n\u001b[32m    145\u001b[39m ) -> Optional[List[Union[SamplerOutput, PoolerOutput]]]:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollective_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexecute_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py:58\u001b[39m, in \u001b[36mUniProcExecutor.collective_rpc\u001b[39m\u001b[34m(self, method, timeout, args, kwargs)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     57\u001b[39m     kwargs = {}\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m answer = \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [answer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/utils/__init__.py:2985\u001b[39m, in \u001b[36mrun_method\u001b[39m\u001b[34m(obj, method, args, kwargs)\u001b[39m\n\u001b[32m   2983\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2984\u001b[39m     func = partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2985\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py:417\u001b[39m, in \u001b[36mLocalOrDistributedWorkerBase.execute_model\u001b[39m\u001b[34m(self, execute_model_req)\u001b[39m\n\u001b[32m    412\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.observability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    413\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.observability_config.collect_model_execute_time):\n\u001b[32m    414\u001b[39m         orig_model_execute_time = intermediate_tensors.tensors.get(\n\u001b[32m    415\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel_execute_time\u001b[39m\u001b[33m\"\u001b[39m, torch.tensor(\u001b[32m0\u001b[39m)).item()\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker_input\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvirtual_engine\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    426\u001b[39m model_execute_time = time.perf_counter() - start_time\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group().is_last_rank:\n\u001b[32m    428\u001b[39m     \u001b[38;5;66;03m# output is IntermediateTensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/worker/model_runner.py:1703\u001b[39m, in \u001b[36mModelRunner.execute_model\u001b[39m\u001b[34m(self, model_input, kv_caches, intermediate_tensors, num_steps, **kwargs)\u001b[39m\n\u001b[32m   1700\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m bypass_model_exec:\n\u001b[32m   1701\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_forward_context(model_input.attn_metadata,\n\u001b[32m   1702\u001b[39m                              \u001b[38;5;28mself\u001b[39m.vllm_config, virtual_engine):\n\u001b[32m-> \u001b[39m\u001b[32m1703\u001b[39m         hidden_or_intermediate_states = \u001b[43mmodel_executable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1704\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1705\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m.\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput_positions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m            \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mMultiModalKwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_kwargs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmulti_modal_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1710\u001b[39m \u001b[43m                \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1711\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1712\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mseqlen_agnostic_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1713\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1714\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1716\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.observability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1717\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.observability_config.collect_model_forward_time):\n\u001b[32m   1718\u001b[39m     model_forward_end.record()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:584\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, positions, intermediate_tensors, inputs_embeds)\u001b[39m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    578\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    579\u001b[39m     input_ids: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    582\u001b[39m     inputs_embeds: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    583\u001b[39m ) -> Union[torch.Tensor, IntermediateTensors]:\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m     model_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m                              \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    586\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/compilation/decorators.py:206\u001b[39m, in \u001b[36m_support_torch_compile.<locals>.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    202\u001b[39m     \u001b[38;5;66;03m# torch.compiler.is_compiling() means we are inside the compilation\u001b[39;00m\n\u001b[32m    203\u001b[39m     \u001b[38;5;66;03m# e.g. TPU has the compilation logic in model runner, so we don't\u001b[39;00m\n\u001b[32m    204\u001b[39m     \u001b[38;5;66;03m# need to compile the model inside.\u001b[39;00m\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.do_not_compile \u001b[38;5;129;01mor\u001b[39;00m torch.compiler.is_compiling():\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m     \u001b[38;5;66;03m# the first compilation needs to have dynamic shapes marked\u001b[39;00m\n\u001b[32m    209\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.compiled_codes) < \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:392\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, positions, intermediate_tensors, inputs_embeds)\u001b[39m\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aux_hidden_state_layers:\n\u001b[32m    391\u001b[39m         aux_hidden_states.append(hidden_states + residual)\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m     hidden_states, residual = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group().is_last_rank:\n\u001b[32m    395\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m IntermediateTensors({\n\u001b[32m    396\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhidden_states\u001b[39m\u001b[33m\"\u001b[39m: hidden_states,\n\u001b[32m    397\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mresidual\u001b[39m\u001b[33m\"\u001b[39m: residual\n\u001b[32m    398\u001b[39m     })\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:305\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, positions, hidden_states, residual)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    303\u001b[39m     hidden_states, residual = \u001b[38;5;28mself\u001b[39m.input_layernorm(\n\u001b[32m    304\u001b[39m         hidden_states, residual)\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m                               \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[32m    309\u001b[39m hidden_states, residual = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(\n\u001b[32m    310\u001b[39m     hidden_states, residual)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:203\u001b[39m, in \u001b[36mLlamaAttention.forward\u001b[39m\u001b[34m(self, positions, hidden_states)\u001b[39m\n\u001b[32m    201\u001b[39m q, k, v = qkv.split([\u001b[38;5;28mself\u001b[39m.q_size, \u001b[38;5;28mself\u001b[39m.kv_size, \u001b[38;5;28mself\u001b[39m.kv_size], dim=-\u001b[32m1\u001b[39m)\n\u001b[32m    202\u001b[39m q, k = \u001b[38;5;28mself\u001b[39m.rotary_emb(positions, q, k)\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m attn_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m output, _ = \u001b[38;5;28mself\u001b[39m.o_proj(attn_output)\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/attention/layer.py:288\u001b[39m, in \u001b[36mAttention.forward\u001b[39m\u001b[34m(self, query, key, value, output_shape)\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.impl.forward(\u001b[38;5;28mself\u001b[39m, query, key, value,\n\u001b[32m    286\u001b[39m                              self_kv_cache, attn_metadata)\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43munified_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/torch/_ops.py:1158\u001b[39m, in \u001b[36mOpOverloadPacket.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[32m   1157\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/attention/layer.py:448\u001b[39m, in \u001b[36munified_attention\u001b[39m\u001b[34m(query, key, value, layer_name)\u001b[39m\n\u001b[32m    446\u001b[39m \u001b[38;5;28mself\u001b[39m = forward_context.no_compile_layers[layer_name]\n\u001b[32m    447\u001b[39m kv_cache = \u001b[38;5;28mself\u001b[39m.kv_cache[forward_context.virtual_engine]\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m maybe_save_kv_layer_to_connector(layer_name, kv_cache)\n\u001b[32m    452\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/attention/backends/xformers.py:584\u001b[39m, in \u001b[36mXFormersImpl.forward\u001b[39m\u001b[34m(self, layer, query, key, value, kv_cache, attn_metadata, output, output_scale)\u001b[39m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m prefill_meta.max_query_len \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    580\u001b[39m \u001b[38;5;66;03m# prefix-enabled attention\u001b[39;00m\n\u001b[32m    581\u001b[39m \u001b[38;5;66;03m# TODO(Hai) this triton kernel has regression issue (broke) to\u001b[39;00m\n\u001b[32m    582\u001b[39m \u001b[38;5;66;03m# deal with different data types between KV and FP8 KV cache,\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;66;03m# to be addressed separately.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m out = \u001b[43mPagedAttention\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward_prefix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkv_cache_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefill_meta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mblock_tables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefill_meta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery_start_loc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefill_meta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseq_lens_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefill_meta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_query_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43malibi_slopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_k_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_v_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m output[:num_prefill_query_tokens].shape == out.shape\n\u001b[32m    601\u001b[39m output[:num_prefill_query_tokens] = out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/attention/ops/paged_attn.py:214\u001b[39m, in \u001b[36mPagedAttention.forward_prefix\u001b[39m\u001b[34m(query, key, value, kv_cache_dtype, key_cache, value_cache, block_tables, query_start_loc, seq_lens_tensor, max_query_len, alibi_slopes, sliding_window, k_scale, v_scale)\u001b[39m\n\u001b[32m    212\u001b[39m output = torch.empty_like(query)\n\u001b[32m    213\u001b[39m max_seq_len = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m \u001b[43mcontext_attention_fwd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkv_cache_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_tables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# query_start_loc is (batch_size + 1,)\u001b[39;49;00m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_start_loc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseq_lens_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_query_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mv_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43malibi_slopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/attention/ops/prefix_prefill.py:850\u001b[39m, in \u001b[36mcontext_attention_fwd\u001b[39m\u001b[34m(q, k, v, o, kv_cache_dtype, k_cache, v_cache, b_loc, b_start_loc, b_seq_len, max_seq_len, max_input_len, k_scale, v_scale, alibi_slopes, sliding_window, sm_scale, skip_decode)\u001b[39m\n\u001b[32m    846\u001b[39m     extra_kargs = {\u001b[33m\"\u001b[39m\u001b[33mkpack\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m2\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwaves_per_eu\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m2\u001b[39m}\n\u001b[32m    848\u001b[39m grid = \u001b[38;5;28;01mlambda\u001b[39;00m META: (batch, head,\n\u001b[32m    849\u001b[39m                      triton.cdiv(max_input_len, META[\u001b[33m\"\u001b[39m\u001b[33mBLOCK_M\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m--> \u001b[39m\u001b[32m850\u001b[39m \u001b[43m_fwd_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m    \u001b[49m\u001b[43mv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m    \u001b[49m\u001b[43mb_loc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m    \u001b[49m\u001b[43msm_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mv_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43mb_start_loc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m    \u001b[49m\u001b[43mb_seq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m    \u001b[49m\u001b[43mb_loc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m    \u001b[49m\u001b[43mb_loc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m    \u001b[49m\u001b[43mo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m    \u001b[49m\u001b[43mo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43mo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m        \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#[num_blocks, num_kv_heads, head_size/x, block_size, x]\u001b[39;49;00m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mv_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mv_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mv_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mv_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#[num_blocks, num_kv_heads, head_size, block_size]\u001b[39;49;00m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mBLOCK_SIZE\u001b[49m\u001b[43m=\u001b[49m\u001b[43mv_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_queries_per_kv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_queries_per_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mIN_PRECISION\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIN_PRECISION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mBLOCK_DMODEL\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mBLOCK_DMODEL_PADDED\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLk_padded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mSLIDING_WINDOW\u001b[49m\u001b[43m=\u001b[49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mSKIP_DECODE\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_decode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mBLOCK_M\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43mBLOCK_N\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_unroll_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_unroll_request\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_warps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_stages\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mextra_kargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/triton/runtime/jit.py:347\u001b[39m, in \u001b[36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) -> T:\n\u001b[32m    342\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[33;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[32m    344\u001b[39m \u001b[33;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[33;03m    memorizes the grid.\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m *args, **kwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/triton/runtime/jit.py:569\u001b[39m, in \u001b[36mJITFunction.run\u001b[39m\u001b[34m(self, grid, warmup, *args, **kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m \u001b[38;5;66;03m# compile the kernel\u001b[39;00m\n\u001b[32m    568\u001b[39m src = \u001b[38;5;28mself\u001b[39m.ASTSource(\u001b[38;5;28mself\u001b[39m, signature, constexprs, attrs)\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m kernel = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__dict__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m kernel_cache[key] = kernel\n\u001b[32m    571\u001b[39m \u001b[38;5;28mself\u001b[39m._call_hook(key, signature, device, constexprs, options, [attrs], warmup, before=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/triton/compiler/compiler.py:284\u001b[39m, in \u001b[36mcompile\u001b[39m\u001b[34m(src, target, options)\u001b[39m\n\u001b[32m    282\u001b[39m use_ir_loc = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mUSE_IR_LOC\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ext, compile_ir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(stages.items())[first_stage:]:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     next_module = \u001b[43mcompile_ir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m     ir_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (fn_override_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (full_name := fn_override_manager.get_file(ir_filename)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/triton/backends/nvidia/compiler.py:450\u001b[39m, in \u001b[36mCUDABackend.add_stages.<locals>.<lambda>\u001b[39m\u001b[34m(src, metadata)\u001b[39m\n\u001b[32m    448\u001b[39m stages[\u001b[33m\"\u001b[39m\u001b[33mttir\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mlambda\u001b[39;00m src, metadata: \u001b[38;5;28mself\u001b[39m.make_ttir(src, metadata, options)\n\u001b[32m    449\u001b[39m stages[\u001b[33m\"\u001b[39m\u001b[33mttgir\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mlambda\u001b[39;00m src, metadata: \u001b[38;5;28mself\u001b[39m.make_ttgir(src, metadata, options, capability)\n\u001b[32m--> \u001b[39m\u001b[32m450\u001b[39m stages[\u001b[33m\"\u001b[39m\u001b[33mllir\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mlambda\u001b[39;00m src, metadata: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmake_llir\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapability\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m stages[\u001b[33m\"\u001b[39m\u001b[33mptx\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mlambda\u001b[39;00m src, metadata: \u001b[38;5;28mself\u001b[39m.make_ptx(src, metadata, options, \u001b[38;5;28mself\u001b[39m.target.arch)\n\u001b[32m    452\u001b[39m stages[\u001b[33m\"\u001b[39m\u001b[33mcubin\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mlambda\u001b[39;00m src, metadata: \u001b[38;5;28mself\u001b[39m.make_cubin(src, metadata, options, \u001b[38;5;28mself\u001b[39m.target.arch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/triton/backends/nvidia/compiler.py:341\u001b[39m, in \u001b[36mCUDABackend.make_llir\u001b[39m\u001b[34m(self, src, metadata, options, capability)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mTRITON_DISABLE_LINE_INFO\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    340\u001b[39m     passes.llvmir.add_di_scope(pm)\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m \u001b[43mpm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[38;5;66;03m# LLVM-IR (MLIR) -> LLVM-IR (LLVM)\u001b[39;00m\n\u001b[32m    343\u001b[39m llvm.init_targets()\n",
      "\u001b[31mRuntimeError\u001b[39m: PassManager::run failed"
     ]
    }
   ],
   "source": [
    "cases = [p.to_case() for p in load_gsm_eng()]\n",
    "response_model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "agent_evaluated = HuggingFaceAgent(response_model_name, examples=cases[-3:], answer_pattern=answer_pattern)\n",
    "for case in cases[:2]:\n",
    "    print(\"Case:\")\n",
    "    print(case)\n",
    "    print(\"Output:\")\n",
    "    print(agent_evaluated.run(case.inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d754afea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2481197568\n",
      "2650800128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c097bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", torch_dtype=\"auto\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56655ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3474a8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Case(name='728', inputs='Candy has 15 light blue spools of thread, 45 dark blue spools of thread, 40 light green spools of thread, and 50 dark green spools of thread. What percent of her spools are blue?', metadata={'filepath': '/home/au338890/repos/m-gsm-symbolic/data/templates/eng/symbolic/0066.json'}, expected_output='First find the number of blue spools: 15 spools + 45 spools = <<15+45=60>>60 spools\\nThen find the total number of spools: 40 spools + 50 spools + 60 spools = <<40+50+60=150>>150 spools\\nThen divide the number of blue spools by the total number of spools and multiply by 100% to express the answer as a percentage: 60 spools / 150 spools * 100% = 40%\\n#### 40', evaluators=[])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b61d6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_evaluated = HuggingFaceAgent(response_model_name, examples=cases[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "393b0bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3672, in run_code\n",
      "  |     exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  |     ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/tmp/ipykernel_10327/1812324380.py\", line 5, in <module>\n",
      "  |     report_2 = ds.evaluate_sync(answer_question)\n",
      "  |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 315, in evaluate_sync\n",
      "  |     return get_event_loop().run_until_complete(self.evaluate(task, name=name, max_concurrency=max_concurrency))\n",
      "  |            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/nest_asyncio.py\", line 98, in run_until_complete\n",
      "  |     return f.result()\n",
      "  |            ~~~~~~~~^^\n",
      "  |   File \"/home/au338890/.local/share/uv/python/cpython-3.13.3-linux-x86_64-gnu/lib/python3.13/asyncio/futures.py\", line 199, in result\n",
      "  |     raise self._exception.with_traceback(self._exception_tb)\n",
      "  |   File \"/home/au338890/.local/share/uv/python/cpython-3.13.3-linux-x86_64-gnu/lib/python3.13/asyncio/tasks.py\", line 306, in __step_run_and_handle_result\n",
      "  |     result = coro.throw(exc)\n",
      "  |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 283, in evaluate\n",
      "  |     cases=await task_group_gather(\n",
      "  |           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |     ...<4 lines>...\n",
      "  |     ),\n",
      "  |     ^\n",
      "  |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/_utils.py\", line 99, in task_group_gather\n",
      "  |     async with anyio.create_task_group() as tg:\n",
      "  |                ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  |         \"unhandled errors in a TaskGroup\", self._exceptions\n",
      "  |     ) from None\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (2 sub-exceptions)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/home/au338890/.local/share/uv/python/cpython-3.13.3-linux-x86_64-gnu/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
      "    |     result = coro.send(None)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/_utils.py\", line 97, in _run_task\n",
      "    |     results[index] = await tsk()\n",
      "    |                      ^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 279, in _handle_case\n",
      "    |     return await _run_task_and_evaluators(task, case, report_case_name, self.evaluators)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 882, in _run_task_and_evaluators\n",
      "    |     scoring_context = await _run_task(task, case)\n",
      "    |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 820, in _run_task\n",
      "    |     task_output = await task(case.inputs)\n",
      "    |                   ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/tmp/ipykernel_10327/1812324380.py\", line 2, in answer_question\n",
      "    |     r = agent_evaluated.run(question)\n",
      "    |   File \"/tmp/ipykernel_10327/3741609218.py\", line 25, in run\n",
      "    |     model_output = self.model.generate(**model_input, max_new_tokens=500)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    |     return func(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/generation/utils.py\", line 2633, in generate\n",
      "    |     result = self._sample(\n",
      "    |         input_ids,\n",
      "    |     ...<5 lines>...\n",
      "    |         **model_kwargs,\n",
      "    |     )\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/generation/utils.py\", line 3614, in _sample\n",
      "    |     outputs = self(**model_inputs, return_dict=True)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    |     return self._call_impl(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    |     return forward_call(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/utils/generic.py\", line 961, in wrapper\n",
      "    |     output = func(self, *args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py\", line 460, in forward\n",
      "    |     outputs: BaseModelOutputWithPast = self.model(\n",
      "    |                                        ~~~~~~~~~~^\n",
      "    |         input_ids=input_ids,\n",
      "    |         ^^^^^^^^^^^^^^^^^^^^\n",
      "    |     ...<6 lines>...\n",
      "    |         **kwargs,\n",
      "    |         ^^^^^^^^^\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    |     return self._call_impl(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    |     return forward_call(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/utils/generic.py\", line 1069, in wrapper\n",
      "    |     outputs = func(self, *args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py\", line 390, in forward\n",
      "    |     hidden_states = decoder_layer(\n",
      "    |         hidden_states,\n",
      "    |     ...<5 lines>...\n",
      "    |         **kwargs,\n",
      "    |     )\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/modeling_layers.py\", line 94, in __call__\n",
      "    |     return super().__call__(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    |     return self._call_impl(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    |     return forward_call(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py\", line 289, in forward\n",
      "    |     hidden_states, _ = self.self_attn(\n",
      "    |                        ~~~~~~~~~~~~~~^\n",
      "    |         hidden_states=hidden_states,\n",
      "    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |     ...<6 lines>...\n",
      "    |         **kwargs,\n",
      "    |         ^^^^^^^^^\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    |     return self._call_impl(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    |     return forward_call(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py\", line 248, in forward\n",
      "    |     attn_output, attn_weights = attention_interface(\n",
      "    |                                 ~~~~~~~~~~~~~~~~~~~^\n",
      "    |         self,\n",
      "    |         ^^^^^\n",
      "    |     ...<6 lines>...\n",
      "    |         **kwargs,\n",
      "    |         ^^^^^^^^^\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/integrations/sdpa_attention.py\", line 81, in sdpa_attention_forward\n",
      "    |     attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "    |         query,\n",
      "    |     ...<6 lines>...\n",
      "    |         **sdpa_kwargs,\n",
      "    |     )\n",
      "    | RuntimeError: [enforce fail at alloc_cpu.cpp:119] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 35004463232 bytes. Error code 12 (Cannot allocate memory)\n",
      "    +---------------- 2 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/home/au338890/.local/share/uv/python/cpython-3.13.3-linux-x86_64-gnu/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
      "    |     result = coro.send(None)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/_utils.py\", line 97, in _run_task\n",
      "    |     results[index] = await tsk()\n",
      "    |                      ^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 279, in _handle_case\n",
      "    |     return await _run_task_and_evaluators(task, case, report_case_name, self.evaluators)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 882, in _run_task_and_evaluators\n",
      "    |     scoring_context = await _run_task(task, case)\n",
      "    |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 820, in _run_task\n",
      "    |     task_output = await task(case.inputs)\n",
      "    |                   ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/tmp/ipykernel_10327/1812324380.py\", line 2, in answer_question\n",
      "    |     r = agent_evaluated.run(question)\n",
      "    |   File \"/tmp/ipykernel_10327/3741609218.py\", line 25, in run\n",
      "    |     model_output = self.model.generate(**model_input, max_new_tokens=500)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    |     return func(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/generation/utils.py\", line 2633, in generate\n",
      "    |     result = self._sample(\n",
      "    |         input_ids,\n",
      "    |     ...<5 lines>...\n",
      "    |         **model_kwargs,\n",
      "    |     )\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/generation/utils.py\", line 3614, in _sample\n",
      "    |     outputs = self(**model_inputs, return_dict=True)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    |     return self._call_impl(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    |     return forward_call(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/utils/generic.py\", line 961, in wrapper\n",
      "    |     output = func(self, *args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py\", line 460, in forward\n",
      "    |     outputs: BaseModelOutputWithPast = self.model(\n",
      "    |                                        ~~~~~~~~~~^\n",
      "    |         input_ids=input_ids,\n",
      "    |         ^^^^^^^^^^^^^^^^^^^^\n",
      "    |     ...<6 lines>...\n",
      "    |         **kwargs,\n",
      "    |         ^^^^^^^^^\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    |     return self._call_impl(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    |     return forward_call(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/utils/generic.py\", line 1069, in wrapper\n",
      "    |     outputs = func(self, *args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py\", line 390, in forward\n",
      "    |     hidden_states = decoder_layer(\n",
      "    |         hidden_states,\n",
      "    |     ...<5 lines>...\n",
      "    |         **kwargs,\n",
      "    |     )\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/modeling_layers.py\", line 94, in __call__\n",
      "    |     return super().__call__(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    |     return self._call_impl(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    |     return forward_call(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py\", line 289, in forward\n",
      "    |     hidden_states, _ = self.self_attn(\n",
      "    |                        ~~~~~~~~~~~~~~^\n",
      "    |         hidden_states=hidden_states,\n",
      "    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |     ...<6 lines>...\n",
      "    |         **kwargs,\n",
      "    |         ^^^^^^^^^\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    |     return self._call_impl(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    |     return forward_call(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py\", line 248, in forward\n",
      "    |     attn_output, attn_weights = attention_interface(\n",
      "    |                                 ~~~~~~~~~~~~~~~~~~~^\n",
      "    |         self,\n",
      "    |         ^^^^^\n",
      "    |     ...<6 lines>...\n",
      "    |         **kwargs,\n",
      "    |         ^^^^^^^^^\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/integrations/sdpa_attention.py\", line 81, in sdpa_attention_forward\n",
      "    |     attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "    |         query,\n",
      "    |     ...<6 lines>...\n",
      "    |         **sdpa_kwargs,\n",
      "    |     )\n",
      "    | RuntimeError: [enforce fail at alloc_cpu.cpp:119] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 35148549248 bytes. Error code 12 (Cannot allocate memory)\n",
      "    +------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "async def answer_question(question: str) -> str:\n",
    "    r = agent_evaluated.run(question)\n",
    "    return r\n",
    "\n",
    "report_2 = ds.evaluate_sync(answer_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ff9ec58",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'report_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mreport_2\u001b[49m.print(include_input=\u001b[38;5;28;01mTrue\u001b[39;00m, include_output=\u001b[38;5;28;01mTrue\u001b[39;00m, include_expected_output=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'report_2' is not defined"
     ]
    }
   ],
   "source": [
    "report_2.print(include_input=True, include_output=True, include_expected_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f208c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4517bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hvis hver pirat svarede sandt til den givne opgave, skal et antal mnter, der er tilfldigt resulterer i et antal mnter, der er lik 30.\n",
      "I et ufuldstndigt skattecontainer, skal du vurdere, hvor mange mnter, der er i containeren, der er i den rigtige mnter.\n",
      "Antallet af mnter, der er i den rigtige mnter, er 10.\n",
      "\n",
      "Fr du tager beslutningen, skal du udfre en fejlfindende analyse.\n",
      "Tlle alle mnterne i containeren, der er i den rigtige mnter.\n",
      "Antallet af mnter, der er i den rigtige mnter, er 10.\n",
      "Der er 6 mnter, der er i den rigtige mnter.\n",
      "Antallet af mnter, der er i den rigtige mnter, er 10.\n",
      "Antallet af mnter, der er i den rigtige mnter, er 10.\n",
      "\n",
      "Det er ikke sandt, at der er 10 mnter i den rigtige mnter.\n",
      "Antallet af mnter i den rigtige mnter er 10.\n",
      "For at bestemme, hvor mange mnter, der er i den rigtige mnter, er det mest almindelige.\n",
      "Tll alle mnterne, der er i containeren, der er i den rigtige mnter.\n",
      "|     | Gold | Silver | Bronze |\n",
      "| Tom |   9   |   11   |   12   |\n",
      "| Al  |  7   |   10   |   10   |\n",
      "| Pit |  10  |   10   |   10   |\n",
      "| Jim |  9   |   10   |   10   |\n",
      "\n",
      "|     | Gold | Silver | Bronze |\n",
      "| Tom |   9   |   11   |   12   |\n",
      "| Al  |  7   |   10   |   10   |\n",
      "| Pit |  10  |   10   |   10   \n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b922864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output does not provide the correct solution to the problem. It repeats incorrect numbers for the coins and does not solve the logic puzzle based on the condition given (only one pirate tells the truth).\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0].assertion_reason)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m-gsm-symbolic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
