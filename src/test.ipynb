{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d442606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 11:21:57 [__init__.py:235] Automatically detected platform cuda.\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import argparse\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "#import outlines\n",
    "#from outlines.types.dsl import Regex, String, at_most, either, to_regex\n",
    "#from outlines.types import zero_or_more, one_or_more, optional, whitespace, digit\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from pydantic_evals import Dataset\n",
    "from pydantic_evals.evaluators import Evaluator, EvaluatorContext\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.sampling_params import GuidedDecodingParams\n",
    "\n",
    "from m_gsm_symbolic.kaenguruen.load_data import load_kaenguruen\n",
    "from m_gsm_symbolic.load_data import load_gsm_dan, load_gsm_eng\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ab570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c58e0985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "#load_dotenv()\n",
    "#api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "from huggingface_hub import login\n",
    "#login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5dc05c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#judge_llm_name = \"openai:gpt-4o-2024-08-06\"\n",
    "response_model_name = \"meta-llama/Llama-3.2-1B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee20a52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Outlines regex DSL: reasoning (any text) + '####' + answer (int)\n",
    "#answer_pattern = to_regex(\n",
    "#    Regex(pattern=r\".\").at_most(500) +\n",
    "#    String(\"#### \") +\n",
    "#    digit.one_or_more() +\n",
    "#    optional(\n",
    "#        either(\n",
    "#            String(\".\"), String(\",\")\n",
    "#        ) + optional(\n",
    "#            digit.one_or_more()\n",
    "#        )\n",
    "#    )\n",
    "#)\n",
    "answer_pattern = r\"[\\s\\S]{0,300}####\\s\\d+\"\n",
    "# Support float or frac as well\n",
    "    #sequence(\n",
    "    #    one_or_more(digit()),\n",
    "    #    optional(either(\n",
    "    #        sequence(literal(\".\"), one_or_more(digit())),\n",
    "    #        sequence(literal(\"/\"), one_or_more(digit()))\n",
    "    #    ))\n",
    "    #)\n",
    "\n",
    "class HuggingFaceAgent:\n",
    "    def __init__(self, model: str, examples: list, \n",
    "                 answer_pattern: str = answer_pattern):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        #self.model = outlines.from_transformers(AutoModelForCausalLM.from_pretrained(model, torch_dtype=\"auto\").to(device).eval(), self.tokenizer)\n",
    "        \n",
    "        #self.model = AutoModelForCausalLM.from_pretrained(model, torch_dtype=\"auto\").to(device)\n",
    "        self.model = LLM(model=model)\n",
    "        self.cases = examples\n",
    "        self.answer_pattern = answer_pattern\n",
    "\n",
    "    def _build_prompt(self, prompt):\n",
    "        examples = []\n",
    "        for case in self.cases:\n",
    "            example = f\"Problem: {case.inputs}\\n\\nSolution: {case.expected_output}\"\n",
    "            examples.append(example)\n",
    "\n",
    "        prompt = f\"Problem: {prompt}\\n\\nSolution:\"\n",
    "        examples.append(prompt)\n",
    "        prompt = \"\\n\\n\".join(examples)\n",
    "        return prompt\n",
    "\n",
    "    def run(self, prompt: str):\n",
    "        prompt = self._build_prompt(prompt)\n",
    "        guided_params = GuidedDecodingParams(regex=self.answer_pattern)\n",
    "        sampling_params = SamplingParams(guided_decoding=guided_params, max_tokens=500)\n",
    "        model_output = self.model.generate(\n",
    "            prompt,\n",
    "            sampling_params=sampling_params\n",
    "        )\n",
    "        #model_output = self.model(prompt, answer_pattern, max_new_tokens=500)\n",
    "        #inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        #inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        #model_output = self.model.generate(**inputs, max_new_tokens=500)\n",
    "        #model_output = self.tokenizer.decode(model_output[0], skip_special_tokens=True)\n",
    "        return model_output[0].outputs[0].text\n",
    "\n",
    "# 1. Outlines regex DSL: reasoning (any text) + '####' + answer (int)\n",
    "#answer_pattern = to_regex(\n",
    "#    Regex(pattern=r\".\").at_most(500) +\n",
    "#    String(\"#### \") +\n",
    "#    digit.one_or_more() +\n",
    "#    optional(\n",
    "#        either(\n",
    "#            String(\".\"), String(\",\")\n",
    "#        ) + optional(\n",
    "#            digit.one_or_more()\n",
    "#        )\n",
    "#    )\n",
    "#)\n",
    "answer_pattern = r\"[\\s\\S]{0,300}####\\s\\d+\"\n",
    "# Support float or frac as well\n",
    "    #sequence(\n",
    "    #    one_or_more(digit()),\n",
    "    #    optional(either(\n",
    "    #        sequence(literal(\".\"), one_or_more(digit())),\n",
    "    #        sequence(literal(\"/\"), one_or_more(digit()))\n",
    "    #    ))\n",
    "    #)\n",
    "\n",
    "class HuggingFaceAgent:\n",
    "    def __init__(self, model: str, examples: list, \n",
    "                 answer_pattern: str = answer_pattern):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        #self.model = outlines.from_transformers(AutoModelForCausalLM.from_pretrained(model, torch_dtype=\"auto\").to(device).eval(), self.tokenizer)\n",
    "        \n",
    "        #self.model = AutoModelForCausalLM.from_pretrained(model, torch_dtype=\"auto\").to(device)\n",
    "        self.model = LLM(model=model)\n",
    "        self.cases = examples\n",
    "        self.answer_pattern = answer_pattern\n",
    "\n",
    "    def _build_prompt(self, prompt):\n",
    "        examples = []\n",
    "        for case in self.cases:\n",
    "            example = f\"Problem: {case.inputs}\\n\\nSolution: {case.expected_output}\"\n",
    "            examples.append(example)\n",
    "\n",
    "        prompt = f\"Problem: {prompt}\\n\\nSolution:\"\n",
    "        examples.append(prompt)\n",
    "        prompt = \"\\n\\n\".join(examples)\n",
    "        return prompt\n",
    "\n",
    "    def run(self, prompt: str):\n",
    "        prompt = self._build_prompt(prompt)\n",
    "        guided_params = GuidedDecodingParams(regex=self.answer_pattern)\n",
    "        sampling_params = SamplingParams(guided_decoding=guided_params, max_tokens=500)\n",
    "        model_output = self.model.generate(\n",
    "            prompt,\n",
    "            sampling_params=sampling_params\n",
    "        )\n",
    "        #model_output = self.model(prompt, answer_pattern, max_new_tokens=500)\n",
    "        #inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        #inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        #model_output = self.model.generate(**inputs, max_new_tokens=500)\n",
    "        #model_output = self.tokenizer.decode(model_output[0], skip_special_tokens=True)\n",
    "        return model_output[0].outputs[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0143d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 0: 40\n",
      "Case 1: 58\n",
      "Case 2: 30\n",
      "Case 3: 38\n",
      "Case 4: 56\n",
      "Case 5: 4\n",
      "Case 6: 18\n",
      "Case 7: 54\n",
      "Case 8: 210\n",
      "Case 9: 1050\n",
      "Case 10: 8\n",
      "Case 11: 4\n",
      "Case 12: 5\n",
      "Case 13: 55\n",
      "Case 14: 2\n",
      "Case 15: 11\n",
      "Case 16: 77\n",
      "Case 17: 118000\n",
      "Case 18: 16\n",
      "Case 19: 540\n",
      "Case 20: 35\n",
      "Case 21: 43200\n",
      "Case 22: 64\n",
      "Case 23: 12\n",
      "Case 24: 88\n",
      "Case 25: 10\n",
      "Case 26: 130\n",
      "Case 27: 342\n",
      "Case 28: 70\n",
      "Case 29: 320\n",
      "Case 30: 157\n",
      "Case 31: 2\n",
      "Case 32: 25\n",
      "Case 33: 72\n",
      "Case 34: 75\n",
      "Case 35: 140\n",
      "Case 36: 16\n",
      "Case 37: 14400\n",
      "Case 38: 2\n",
      "Case 39: 3200\n",
      "Case 40: 100\n",
      "Case 41: 5\n",
      "Case 42: 14\n",
      "Case 43: 4\n",
      "Case 44: 63\n",
      "Case 45: 2\n",
      "Case 46: 15\n",
      "Case 47: 200\n",
      "Case 48: 4\n",
      "Case 49: 694\n",
      "Case 50: 4000\n",
      "Case 51: 6\n",
      "Case 52: 9\n",
      "Case 53: 159\n",
      "Case 54: 48\n",
      "Case 55: 45\n",
      "Case 56: 100\n",
      "Case 57: 3\n",
      "Case 58: 42\n",
      "Case 59: 48\n",
      "Case 60: 428\n",
      "Case 61: 25\n",
      "Case 62: 75\n",
      "Case 63: 10\n",
      "Case 64: 3430\n",
      "Case 65: 18\n",
      "Case 66: 78\n",
      "Case 67: 16\n",
      "Case 68: 8\n",
      "Case 69: 49\n",
      "Case 70: 3\n",
      "Case 71: 20\n",
      "Case 72: 34\n",
      "Case 73: 36\n",
      "Case 74: 48\n",
      "Case 75: 1248\n",
      "Case 76: 9500\n",
      "Case 77: 16\n",
      "Case 78: 2\n",
      "Case 79: 36\n",
      "Case 80: 98\n",
      "Case 81: 30\n",
      "Case 82: 44\n",
      "Case 83: 30\n",
      "Case 84: 7\n",
      "Case 85: 78\n",
      "Case 86: 2\n",
      "Case 87: 19\n",
      "Case 88: 2325\n",
      "Case 89: 13\n",
      "Case 90: 750\n",
      "Case 91: 12\n",
      "Case 92: 22\n",
      "Case 93: 20\n",
      "Case 94: 15\n",
      "Case 95: 20\n",
      "Case 96: 10\n",
      "Case 97: 50\n",
      "Case 98: 11\n",
      "Case 99: 50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cases = [p.to_case() for p in load_gsm_eng()]\n",
    "\n",
    "for i, case in enumerate(cases):\n",
    "    print(f\"Case {i}: {re.search(r'####\\s*((\\d|,)+)', case.expected_output).group(1).replace(',', '')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8729d50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = [p.to_case() for p in load_gsm_eng()]\n",
    "\n",
    "response_model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "agent_evaluated = HuggingFaceAgent(response_model_name, examples=cases[-3:], answer_pattern=answer_pattern)\n",
    "\n",
    "# Custom evaluator: compare only the answer after '####'\n",
    "@dataclass\n",
    "class AnswerOnlyMatch(Evaluator):\n",
    "    def evaluate(self, ctx: EvaluatorContext) -> bool:\n",
    "        # Extract answer after '####' using regex\n",
    "        m_pred = re.search(r\"####\\s*((\\d|,)+)\", ctx.output)\n",
    "        m_true = re.search(r\"####\\s*((\\d|,)+)\", ctx.expected_output)\n",
    "        if not m_pred or not m_true:\n",
    "            return False\n",
    "        return float(m_pred.group(1).replace(\",\",\"\")) == float(m_true.group(1).replace(\",\",\"\"))\n",
    "\n",
    "ds = Dataset(\n",
    "    cases=cases[:2],\n",
    "    evaluators=[AnswerOnlyMatch()],\n",
    ")\n",
    "\n",
    "async def answer_question(question: str) -> str:\n",
    "    r = agent_evaluated.run(question)\n",
    "    return r\n",
    "\n",
    "report = ds.evaluate_sync(answer_question)\n",
    "report.print(include_input=True, include_output=True, include_expected_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "209d5cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 11:22:21 [config.py:3392] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.\n",
      "WARNING 08-11 11:22:21 [config.py:3443] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 08-11 11:22:21 [config.py:1604] Using max model len 131072\n",
      "WARNING 08-11 11:22:21 [arg_utils.py:1690] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "WARNING 08-11 11:22:21 [arg_utils.py:1486] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.\n",
      "INFO 08-11 11:22:24 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 08-11 11:22:24 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 08-11 11:22:27 [cuda.py:346] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 08-11 11:22:27 [cuda.py:395] Using XFormers backend.\n",
      "INFO 08-11 11:22:28 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 08-11 11:22:28 [model_runner.py:1083] Starting to load model meta-llama/Llama-3.2-1B...\n",
      "INFO 08-11 11:22:29 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "INFO 08-11 11:22:29 [weight_utils.py:349] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204ae872da99439ea481b1f0eabea35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 11:22:30 [default_loader.py:262] Loading weights took 1.07 seconds\n",
      "INFO 08-11 11:22:31 [model_runner.py:1115] Model loading took 2.3185 GiB and 1.856800 seconds\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 252.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 243.31 MiB is free. Process 120783 has 43.88 GiB memory in use. Including non-PyTorch memory, this process has 3.14 GiB memory in use. Of the allocated memory 2.89 GiB is allocated by PyTorch, and 44.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m cases = [p.to_case() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m load_gsm_eng()]\n\u001b[32m      2\u001b[39m response_model_name = \u001b[33m\"\u001b[39m\u001b[33mmeta-llama/Llama-3.2-1B\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m agent_evaluated = \u001b[43mHuggingFaceAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcases\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer_pattern\u001b[49m\u001b[43m=\u001b[49m\u001b[43manswer_pattern\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m case \u001b[38;5;129;01min\u001b[39;00m cases[:\u001b[32m2\u001b[39m]:\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCase:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mHuggingFaceAgent.__init__\u001b[39m\u001b[34m(self, model, examples, answer_pattern)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer = AutoTokenizer.from_pretrained(model)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m#self.model = outlines.from_transformers(AutoModelForCausalLM.from_pretrained(model, torch_dtype=\"auto\").to(device).eval(), self.tokenizer)\u001b[39;00m\n\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m#self.model = AutoModelForCausalLM.from_pretrained(model, torch_dtype=\"auto\").to(device)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mself\u001b[39m.cases = examples\n\u001b[32m     33\u001b[39m \u001b[38;5;28mself\u001b[39m.answer_pattern = answer_pattern\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py:273\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, task, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, override_pooler_config, compilation_config, **kwargs)\u001b[39m\n\u001b[32m    243\u001b[39m engine_args = EngineArgs(\n\u001b[32m    244\u001b[39m     model=model,\n\u001b[32m    245\u001b[39m     task=task,\n\u001b[32m   (...)\u001b[39m\u001b[32m    269\u001b[39m     **kwargs,\n\u001b[32m    270\u001b[39m )\n\u001b[32m    272\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    277\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:497\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    494\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv1\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[32m    495\u001b[39m     engine_cls = V1LLMEngine\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:473\u001b[39m, in \u001b[36mLLMEngine.from_vllm_config\u001b[39m\u001b[34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[39m\n\u001b[32m    465\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_vllm_config\u001b[39m(\n\u001b[32m    467\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    471\u001b[39m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    472\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLLMEngine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_executor_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:266\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28mself\u001b[39m.model_executor = executor_class(vllm_config=vllm_config)\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_config.runner_type != \u001b[33m\"\u001b[39m\u001b[33mpooling\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_kv_caches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[38;5;66;03m# If usage stat is enabled, collect relevant info.\u001b[39;00m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_usage_stats_enabled():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:409\u001b[39m, in \u001b[36mLLMEngine._initialize_kv_caches\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Initialize the KV cache in the worker(s).\u001b[39;00m\n\u001b[32m    403\u001b[39m \n\u001b[32m    404\u001b[39m \u001b[33;03mThe workers will determine the number of blocks in both the GPU cache\u001b[39;00m\n\u001b[32m    405\u001b[39m \u001b[33;03mand the swap CPU cache.\u001b[39;00m\n\u001b[32m    406\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    407\u001b[39m start = time.time()\n\u001b[32m    408\u001b[39m num_gpu_blocks, num_cpu_blocks = (\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetermine_num_available_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache_config.num_gpu_blocks_override \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    412\u001b[39m     num_gpu_blocks_override = \u001b[38;5;28mself\u001b[39m.cache_config.num_gpu_blocks_override\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/executor/executor_base.py:104\u001b[39m, in \u001b[36mExecutorBase.determine_num_available_blocks\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdetermine_num_available_blocks\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m     92\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Determine the number of available blocks for the GPU KV cache and\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[33;03m    swappable CPU KV cache.\u001b[39;00m\n\u001b[32m     94\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    102\u001b[39m \u001b[33;03m    appended to.\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollective_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdetermine_num_available_blocks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m     a = \u001b[38;5;28mmin\u001b[39m([r[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results])\n\u001b[32m    106\u001b[39m     b = \u001b[38;5;28mmin\u001b[39m([r[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py:58\u001b[39m, in \u001b[36mUniProcExecutor.collective_rpc\u001b[39m\u001b[34m(self, method, timeout, args, kwargs)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     57\u001b[39m     kwargs = {}\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m answer = \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [answer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/utils/__init__.py:2985\u001b[39m, in \u001b[36mrun_method\u001b[39m\u001b[34m(obj, method, args, kwargs)\u001b[39m\n\u001b[32m   2983\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2984\u001b[39m     func = partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2985\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/worker/worker.py:257\u001b[39m, in \u001b[36mWorker.determine_num_available_blocks\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;66;03m# Execute a forward pass with dummy inputs to profile the memory usage\u001b[39;00m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# of the model.\u001b[39;00m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m memory_profiling(\n\u001b[32m    255\u001b[39m         \u001b[38;5;28mself\u001b[39m.baseline_snapshot,\n\u001b[32m    256\u001b[39m         weights_memory=\u001b[38;5;28mself\u001b[39m.model_runner.model_memory_usage) \u001b[38;5;28;01mas\u001b[39;00m result:\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprofile_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[38;5;28mself\u001b[39m._assert_memory_footprint_increased_during_profiling()\n\u001b[32m    261\u001b[39m memory_for_current_instance = total_gpu_memory * \\\n\u001b[32m    262\u001b[39m     \u001b[38;5;28mself\u001b[39m.cache_config.gpu_memory_utilization\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/worker/model_runner.py:1206\u001b[39m, in \u001b[36mGPUModelRunnerBase.profile_run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1203\u001b[39m max_num_batched_tokens = \\\n\u001b[32m   1204\u001b[39m     \u001b[38;5;28mself\u001b[39m.scheduler_config.max_num_batched_tokens\n\u001b[32m   1205\u001b[39m max_num_seqs = \u001b[38;5;28mself\u001b[39m.scheduler_config.max_num_seqs\n\u001b[32m-> \u001b[39m\u001b[32m1206\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dummy_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_num_batched_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_seqs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/worker/model_runner.py:1332\u001b[39m, in \u001b[36mGPUModelRunnerBase._dummy_run\u001b[39m\u001b[34m(self, max_num_batched_tokens, max_num_seqs)\u001b[39m\n\u001b[32m   1329\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_input.attn_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1330\u001b[39m     model_input.attn_metadata.enable_kv_scales_calculation = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1332\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1333\u001b[39m torch.cuda.synchronize()\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lora_config:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/worker/model_runner.py:1765\u001b[39m, in \u001b[36mModelRunner.execute_model\u001b[39m\u001b[34m(self, model_input, kv_caches, intermediate_tensors, num_steps, **kwargs)\u001b[39m\n\u001b[32m   1762\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_input.inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1763\u001b[39m     \u001b[38;5;28mself\u001b[39m.sampler.include_gpu_probs_tensor = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1765\u001b[39m output: SamplerOutput = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1766\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1767\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m.\u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1768\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1769\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.observability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1770\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.observability_config.collect_model_forward_time\n\u001b[32m   1771\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1772\u001b[39m     model_forward_end.synchronize()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/sampler.py:279\u001b[39m, in \u001b[36mSampler.forward\u001b[39m\u001b[34m(self, logits, sampling_metadata)\u001b[39m\n\u001b[32m    276\u001b[39m logits.div_(sampling_tensors.temperatures.unsqueeze(dim=\u001b[32m1\u001b[39m))\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_top_p_top_k \u001b[38;5;129;01mand\u001b[39;00m flashinfer_top_k_top_p_sampling \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     logits = \u001b[43m_apply_top_k_top_p\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_tensors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtop_ps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m                                \u001b[49m\u001b[43msampling_tensors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtop_ks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_min_p:\n\u001b[32m    283\u001b[39m     logits = _apply_min_p(logits, sampling_tensors.min_ps)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/m-gsm-symbolic/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/sampler.py:397\u001b[39m, in \u001b[36m_apply_top_k_top_p\u001b[39m\u001b[34m(logits, p, k)\u001b[39m\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_apply_top_k_top_p\u001b[39m(\n\u001b[32m    393\u001b[39m     logits: torch.Tensor,\n\u001b[32m    394\u001b[39m     p: torch.Tensor,\n\u001b[32m    395\u001b[39m     k: torch.Tensor,\n\u001b[32m    396\u001b[39m ) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m     logits_sort, logits_idx = \u001b[43mlogits\u001b[49m\u001b[43m.\u001b[49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescending\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m     \u001b[38;5;66;03m# Apply top-k.\u001b[39;00m\n\u001b[32m    400\u001b[39m     top_k_mask = logits_sort.size(\u001b[32m1\u001b[39m) - k.to(torch.long)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 252.00 MiB. GPU 0 has a total capacity of 47.27 GiB of which 243.31 MiB is free. Process 120783 has 43.88 GiB memory in use. Including non-PyTorch memory, this process has 3.14 GiB memory in use. Of the allocated memory 2.89 GiB is allocated by PyTorch, and 44.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "cases = [p.to_case() for p in load_gsm_eng()]\n",
    "response_model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "agent_evaluated = HuggingFaceAgent(response_model_name, examples=cases[-3:], answer_pattern=answer_pattern)\n",
    "for case in cases[:2]:\n",
    "    print(\"Case:\")\n",
    "    print(case)\n",
    "    print(\"Output:\")\n",
    "    print(agent_evaluated.run(case.inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d754afea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2481197568\n",
      "2650800128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c097bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", torch_dtype=\"auto\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56655ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3474a8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Case(name='728', inputs='Candy has 15 light blue spools of thread, 45 dark blue spools of thread, 40 light green spools of thread, and 50 dark green spools of thread. What percent of her spools are blue?', metadata={'filepath': '/home/au338890/repos/m-gsm-symbolic/data/templates/eng/symbolic/0066.json'}, expected_output='First find the number of blue spools: 15 spools + 45 spools = <<15+45=60>>60 spools\\nThen find the total number of spools: 40 spools + 50 spools + 60 spools = <<40+50+60=150>>150 spools\\nThen divide the number of blue spools by the total number of spools and multiply by 100% to express the answer as a percentage: 60 spools / 150 spools * 100% = 40%\\n#### 40', evaluators=[])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b61d6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_evaluated = HuggingFaceAgent(response_model_name, examples=cases[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "393b0bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3672, in run_code\n",
      "  |     exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  |     ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/tmp/ipykernel_10327/1812324380.py\", line 5, in <module>\n",
      "  |     report_2 = ds.evaluate_sync(answer_question)\n",
      "  |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 315, in evaluate_sync\n",
      "  |     return get_event_loop().run_until_complete(self.evaluate(task, name=name, max_concurrency=max_concurrency))\n",
      "  |            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/nest_asyncio.py\", line 98, in run_until_complete\n",
      "  |     return f.result()\n",
      "  |            ~~~~~~~~^^\n",
      "  |   File \"/home/au338890/.local/share/uv/python/cpython-3.13.3-linux-x86_64-gnu/lib/python3.13/asyncio/futures.py\", line 199, in result\n",
      "  |     raise self._exception.with_traceback(self._exception_tb)\n",
      "  |   File \"/home/au338890/.local/share/uv/python/cpython-3.13.3-linux-x86_64-gnu/lib/python3.13/asyncio/tasks.py\", line 306, in __step_run_and_handle_result\n",
      "  |     result = coro.throw(exc)\n",
      "  |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 283, in evaluate\n",
      "  |     cases=await task_group_gather(\n",
      "  |           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |     ...<4 lines>...\n",
      "  |     ),\n",
      "  |     ^\n",
      "  |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/_utils.py\", line 99, in task_group_gather\n",
      "  |     async with anyio.create_task_group() as tg:\n",
      "  |                ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  |         \"unhandled errors in a TaskGroup\", self._exceptions\n",
      "  |     ) from None\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (2 sub-exceptions)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/home/au338890/.local/share/uv/python/cpython-3.13.3-linux-x86_64-gnu/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
      "    |     result = coro.send(None)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/_utils.py\", line 97, in _run_task\n",
      "    |     results[index] = await tsk()\n",
      "    |                      ^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 279, in _handle_case\n",
      "    |     return await _run_task_and_evaluators(task, case, report_case_name, self.evaluators)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 882, in _run_task_and_evaluators\n",
      "    |     scoring_context = await _run_task(task, case)\n",
      "    |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 820, in _run_task\n",
      "    |     task_output = await task(case.inputs)\n",
      "    |                   ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/tmp/ipykernel_10327/1812324380.py\", line 2, in answer_question\n",
      "    |     r = agent_evaluated.run(question)\n",
      "    |   File \"/tmp/ipykernel_10327/3741609218.py\", line 25, in run\n",
      "    |     model_output = self.model.generate(**model_input, max_new_tokens=500)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    |     return func(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/generation/utils.py\", line 2633, in generate\n",
      "    |     result = self._sample(\n",
      "    |         input_ids,\n",
      "    |     ...<5 lines>...\n",
      "    |         **model_kwargs,\n",
      "    |     )\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/generation/utils.py\", line 3614, in _sample\n",
      "    |     outputs = self(**model_inputs, return_dict=True)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    |     return self._call_impl(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    |     return forward_call(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/utils/generic.py\", line 961, in wrapper\n",
      "    |     output = func(self, *args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py\", line 460, in forward\n",
      "    |     outputs: BaseModelOutputWithPast = self.model(\n",
      "    |                                        ~~~~~~~~~~^\n",
      "    |         input_ids=input_ids,\n",
      "    |         ^^^^^^^^^^^^^^^^^^^^\n",
      "    |     ...<6 lines>...\n",
      "    |         **kwargs,\n",
      "    |         ^^^^^^^^^\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    |     return self._call_impl(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    |     return forward_call(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/utils/generic.py\", line 1069, in wrapper\n",
      "    |     outputs = func(self, *args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py\", line 390, in forward\n",
      "    |     hidden_states = decoder_layer(\n",
      "    |         hidden_states,\n",
      "    |     ...<5 lines>...\n",
      "    |         **kwargs,\n",
      "    |     )\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/modeling_layers.py\", line 94, in __call__\n",
      "    |     return super().__call__(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    |     return self._call_impl(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    |     return forward_call(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py\", line 289, in forward\n",
      "    |     hidden_states, _ = self.self_attn(\n",
      "    |                        ~~~~~~~~~~~~~~^\n",
      "    |         hidden_states=hidden_states,\n",
      "    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |     ...<6 lines>...\n",
      "    |         **kwargs,\n",
      "    |         ^^^^^^^^^\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    |     return self._call_impl(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    |     return forward_call(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py\", line 248, in forward\n",
      "    |     attn_output, attn_weights = attention_interface(\n",
      "    |                                 ~~~~~~~~~~~~~~~~~~~^\n",
      "    |         self,\n",
      "    |         ^^^^^\n",
      "    |     ...<6 lines>...\n",
      "    |         **kwargs,\n",
      "    |         ^^^^^^^^^\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/integrations/sdpa_attention.py\", line 81, in sdpa_attention_forward\n",
      "    |     attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "    |         query,\n",
      "    |     ...<6 lines>...\n",
      "    |         **sdpa_kwargs,\n",
      "    |     )\n",
      "    | RuntimeError: [enforce fail at alloc_cpu.cpp:119] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 35004463232 bytes. Error code 12 (Cannot allocate memory)\n",
      "    +---------------- 2 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/home/au338890/.local/share/uv/python/cpython-3.13.3-linux-x86_64-gnu/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
      "    |     result = coro.send(None)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/_utils.py\", line 97, in _run_task\n",
      "    |     results[index] = await tsk()\n",
      "    |                      ^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 279, in _handle_case\n",
      "    |     return await _run_task_and_evaluators(task, case, report_case_name, self.evaluators)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 882, in _run_task_and_evaluators\n",
      "    |     scoring_context = await _run_task(task, case)\n",
      "    |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/pydantic_evals/dataset.py\", line 820, in _run_task\n",
      "    |     task_output = await task(case.inputs)\n",
      "    |                   ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/tmp/ipykernel_10327/1812324380.py\", line 2, in answer_question\n",
      "    |     r = agent_evaluated.run(question)\n",
      "    |   File \"/tmp/ipykernel_10327/3741609218.py\", line 25, in run\n",
      "    |     model_output = self.model.generate(**model_input, max_new_tokens=500)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    |     return func(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/generation/utils.py\", line 2633, in generate\n",
      "    |     result = self._sample(\n",
      "    |         input_ids,\n",
      "    |     ...<5 lines>...\n",
      "    |         **model_kwargs,\n",
      "    |     )\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/generation/utils.py\", line 3614, in _sample\n",
      "    |     outputs = self(**model_inputs, return_dict=True)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    |     return self._call_impl(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    |     return forward_call(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/utils/generic.py\", line 961, in wrapper\n",
      "    |     output = func(self, *args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py\", line 460, in forward\n",
      "    |     outputs: BaseModelOutputWithPast = self.model(\n",
      "    |                                        ~~~~~~~~~~^\n",
      "    |         input_ids=input_ids,\n",
      "    |         ^^^^^^^^^^^^^^^^^^^^\n",
      "    |     ...<6 lines>...\n",
      "    |         **kwargs,\n",
      "    |         ^^^^^^^^^\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    |     return self._call_impl(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    |     return forward_call(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/utils/generic.py\", line 1069, in wrapper\n",
      "    |     outputs = func(self, *args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py\", line 390, in forward\n",
      "    |     hidden_states = decoder_layer(\n",
      "    |         hidden_states,\n",
      "    |     ...<5 lines>...\n",
      "    |         **kwargs,\n",
      "    |     )\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/modeling_layers.py\", line 94, in __call__\n",
      "    |     return super().__call__(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    |     return self._call_impl(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    |     return forward_call(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py\", line 289, in forward\n",
      "    |     hidden_states, _ = self.self_attn(\n",
      "    |                        ~~~~~~~~~~~~~~^\n",
      "    |         hidden_states=hidden_states,\n",
      "    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |     ...<6 lines>...\n",
      "    |         **kwargs,\n",
      "    |         ^^^^^^^^^\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    |     return self._call_impl(*args, **kwargs)\n",
      "    |            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    |     return forward_call(*args, **kwargs)\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py\", line 248, in forward\n",
      "    |     attn_output, attn_weights = attention_interface(\n",
      "    |                                 ~~~~~~~~~~~~~~~~~~~^\n",
      "    |         self,\n",
      "    |         ^^^^^\n",
      "    |     ...<6 lines>...\n",
      "    |         **kwargs,\n",
      "    |         ^^^^^^^^^\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"/home/au338890/repos/m-gsm-symbolic/.venv/lib/python3.13/site-packages/transformers/integrations/sdpa_attention.py\", line 81, in sdpa_attention_forward\n",
      "    |     attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "    |         query,\n",
      "    |     ...<6 lines>...\n",
      "    |         **sdpa_kwargs,\n",
      "    |     )\n",
      "    | RuntimeError: [enforce fail at alloc_cpu.cpp:119] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 35148549248 bytes. Error code 12 (Cannot allocate memory)\n",
      "    +------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "async def answer_question(question: str) -> str:\n",
    "    r = agent_evaluated.run(question)\n",
    "    return r\n",
    "\n",
    "report_2 = ds.evaluate_sync(answer_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ff9ec58",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'report_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mreport_2\u001b[49m.print(include_input=\u001b[38;5;28;01mTrue\u001b[39;00m, include_output=\u001b[38;5;28;01mTrue\u001b[39;00m, include_expected_output=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'report_2' is not defined"
     ]
    }
   ],
   "source": [
    "report_2.print(include_input=True, include_output=True, include_expected_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f208c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4517bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hvis hver pirat svarede sandt til den givne opgave, skal et antal mnter, der er tilfldigt resulterer i et antal mnter, der er lik 30.\n",
      "I et ufuldstndigt skattecontainer, skal du vurdere, hvor mange mnter, der er i containeren, der er i den rigtige mnter.\n",
      "Antallet af mnter, der er i den rigtige mnter, er 10.\n",
      "\n",
      "Fr du tager beslutningen, skal du udfre en fejlfindende analyse.\n",
      "Tlle alle mnterne i containeren, der er i den rigtige mnter.\n",
      "Antallet af mnter, der er i den rigtige mnter, er 10.\n",
      "Der er 6 mnter, der er i den rigtige mnter.\n",
      "Antallet af mnter, der er i den rigtige mnter, er 10.\n",
      "Antallet af mnter, der er i den rigtige mnter, er 10.\n",
      "\n",
      "Det er ikke sandt, at der er 10 mnter i den rigtige mnter.\n",
      "Antallet af mnter i den rigtige mnter er 10.\n",
      "For at bestemme, hvor mange mnter, der er i den rigtige mnter, er det mest almindelige.\n",
      "Tll alle mnterne, der er i containeren, der er i den rigtige mnter.\n",
      "|     | Gold | Silver | Bronze |\n",
      "| Tom |   9   |   11   |   12   |\n",
      "| Al  |  7   |   10   |   10   |\n",
      "| Pit |  10  |   10   |   10   |\n",
      "| Jim |  9   |   10   |   10   |\n",
      "\n",
      "|     | Gold | Silver | Bronze |\n",
      "| Tom |   9   |   11   |   12   |\n",
      "| Al  |  7   |   10   |   10   |\n",
      "| Pit |  10  |   10   |   10   \n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b922864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output does not provide the correct solution to the problem. It repeats incorrect numbers for the coins and does not solve the logic puzzle based on the condition given (only one pirate tells the truth).\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0].assertion_reason)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m-gsm-symbolic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
